import os
import re
import json
import logging
import difflib
import numpy as np
import pandas as pd
import csv
from datetime import datetime
from difflib import SequenceMatcher
import asyncio
from deep_translator import GoogleTranslator
import Levenshtein
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import train_test_split
import psutil
import GPUtil
import networkx as nx
from collections import deque
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader, TensorDataset
from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup
import docx
from PyPDF2 import PdfReader
import warnings
import faiss
from sentence_transformers import SentenceTransformer
import ollama
import pickle
from py2neo import Graph, Node, Relationship

warnings.filterwarnings('ignore')


# ========== LOGGER SETUP ==========
def setup_logger():
    """è®¾ç½®æ—¥å¿—è®°å½•å™¨"""
    log_dir = "logs"
    os.makedirs(log_dir, exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = os.path.join(log_dir, f"medical_diagnosis_{timestamp}.log")

    logging.basicConfig(
        filename=log_file,
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        encoding='utf-8'
    )

    console = logging.StreamHandler()
    console.setLevel(logging.INFO)
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    console.setFormatter(formatter)
    logging.getLogger('').addHandler(console)

    return logging.getLogger('MedicalDiagnosis')


logger = setup_logger()


# ========== é…ç½® ==========
class MedicalConfig:
    def __init__(self):
        self.knowledge_paths = self.setup_knowledge_paths()
        self.model_dir = "trained_models"
        os.makedirs(self.model_dir, exist_ok=True)
        self.viz_dir = "visualizations"
        os.makedirs(self.viz_dir, exist_ok=True)
        self.kg_dir = "knowledge_graphs"
        os.makedirs(self.kg_dir, exist_ok=True)
        self.data_dir = "data"
        os.makedirs(self.data_dir, exist_ok=True)
        self.neo4j_dir = "neo4j_data"
        os.makedirs(self.neo4j_dir, exist_ok=True)

        logger.info(f"æ¨¡å‹ç›®å½•: {self.model_dir}")
        logger.info(f"å¯è§†åŒ–ç›®å½•: {self.viz_dir}")

        # è®­ç»ƒé…ç½®
        self.epochs = 10
        self.batch_size = 8
        self.learning_rate = 2e-5
        self.ddd_threshold = 1.0
        self.warmup_steps = 100
        self.max_grad_norm = 1.0
        self.diagnosis_threshold = 0.95

        # çŸ¥è¯†å›¾è°±é…ç½®
        self.kg_relation_types = [
            "has_symptom", "treated_with", "diagnosed_by",
            "symptom_of", "causes", "prevents", "contraindicates"
        ]

        # æ¨¡å‹é…ç½®
        self.pretrained_model_name = "bert-base-uncased"
        self.hidden_dropout_prob = 0.3
        self.max_seq_length = 128
        self.num_labels = 10

        # æ€è€ƒæ·±åº¦é…ç½®
        self.thinking_depth = 100  # å¢åŠ æ€è€ƒè¿­ä»£æ¬¡æ•°
        self.certainty_threshold = 0.8

        # OLLAMAé…ç½®
        self.ollama_model = "llama2"  # é»˜è®¤ä½¿ç”¨llama2æ¨¡å‹
        self.ollama_base_url = "http://localhost:11434"

        # FAISSé…ç½®
        self.faiss_index_path = os.path.join(self.model_dir, "faiss_index.bin")
        self.embedding_model = "all-MiniLM-L6-v2"

        # Neo4jé…ç½®
        self.neo4j_uri = "bolt://localhost:7687"
        self.neo4j_user = "neo4j"
        self.neo4j_password = "password"
        self.use_neo4j = False  # é»˜è®¤ä¸ä½¿ç”¨Neo4jï¼Œä½¿ç”¨æœ¬åœ°å›¾è°±

        logger.info("\n=== åŒ»ç–—åˆ†æé…ç½® ===")
        logger.info(f"çŸ¥è¯†è·¯å¾„: {self.knowledge_paths}")
        logger.info(f"è¯Šæ–­é˜ˆå€¼: {self.diagnosis_threshold * 100}%")
        logger.info(f"è®­ç»ƒè½®æ•°: {self.epochs}")
        logger.info(f"æ€è€ƒæ·±åº¦: {self.thinking_depth}")
        logger.info(f"OLLAMAæ¨¡å‹: {self.ollama_model}")
        logger.info(f"ä½¿ç”¨Neo4j: {self.use_neo4j}")
        logger.info("===================================")

    def setup_knowledge_paths(self):
        """è®¾ç½®çŸ¥è¯†åº“è·¯å¾„"""
        knowledge_dir = os.path.join(os.getcwd(), "knowledge_base")
        os.makedirs(knowledge_dir, exist_ok=True)
        logger.info(f"çŸ¥è¯†è·¯å¾„: {knowledge_dir}")
        return [knowledge_dir]

    async def translate_to_english(self, text):
        """å¼‚æ­¥ç¿»è¯‘æ–‡æœ¬åˆ°è‹±æ–‡"""
        try:
            return await asyncio.to_thread(GoogleTranslator(source='auto', target='en').translate, text)
        except Exception as e:
            logger.error(f"ç¿»è¯‘é”™è¯¯: {str(e)}")
            return text

    async def translate_to_chinese(self, text):
        """å¼‚æ­¥ç¿»è¯‘æ–‡æœ¬åˆ°ä¸­æ–‡"""
        try:
            return await asyncio.to_thread(GoogleTranslator(source='auto', target='zh-CN').translate, text)
        except Exception as e:
            logger.error(f"ç¿»è¯‘é”™è¯¯: {str(e)}")
            return text

    def is_english(self, text):
        """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦ä¸ºè‹±æ–‡"""
        try:
            text.encode('ascii')
            return True
        except:
            return False

    def is_chinese(self, text):
        """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦ä¸ºä¸­æ–‡"""
        return any('\u4e00' <= char <= '\u9fff' for char in text)

    async def translate_bilingual(self, en_text, cn_text):
        """åˆ›å»ºåŒè¯­æ–‡æœ¬"""
        return f"ğŸŒ ENGLISH:\n{en_text}\n\nğŸŒ ä¸­æ–‡:\n{cn_text}"


# ========== FAISSå‘é‡æ•°æ®åº“ ==========
class FAISSVectorDB:
    def __init__(self, config):
        self.config = config
        self.index = None
        self.embedding_model = SentenceTransformer(config.embedding_model)
        self.documents = []

    def build_index(self, documents):
        """æ„å»ºFAISSç´¢å¼•"""
        self.documents = documents

        # ç”ŸæˆåµŒå…¥å‘é‡
        embeddings = self.embedding_model.encode(documents, show_progress_bar=True)

        # åˆ›å»ºFAISSç´¢å¼•
        dimension = embeddings.shape[1]
        self.index = faiss.IndexFlatL2(dimension)
        self.index.add(embeddings)

        # ä¿å­˜ç´¢å¼•
        faiss.write_index(self.index, self.config.faiss_index_path)
        logger.info(f"FAISSç´¢å¼•å·²æ„å»ºå¹¶ä¿å­˜: {self.config.faiss_index_path}")

    def load_index(self):
        """åŠ è½½FAISSç´¢å¼•"""
        if os.path.exists(self.config.faiss_index_path):
            self.index = faiss.read_index(self.config.faiss_index_path)
            logger.info(f"FAISSç´¢å¼•å·²åŠ è½½: {self.config.faiss_index_path}")
            return True
        return False

    def search(self, query, k=5):
        """æœç´¢ç›¸ä¼¼æ–‡æ¡£"""
        if self.index is None:
            if not self.load_index():
                return []

        # ç”ŸæˆæŸ¥è¯¢åµŒå…¥
        query_embedding = self.embedding_model.encode([query])

        # æœç´¢ç›¸ä¼¼æ–‡æ¡£
        distances, indices = self.index.search(query_embedding, k)

        # è¿”å›ç»“æœ
        results = []
        for i, idx in enumerate(indices[0]):
            if idx < len(self.documents):
                results.append({
                    'document': self.documents[idx],
                    'distance': distances[0][i]
                })

        return results


# ========== æœ¬åœ°çŸ¥è¯†å›¾è°±ç³»ç»Ÿ ==========
class LocalKnowledgeGraph:
    """æœ¬åœ°çŸ¥è¯†å›¾è°±å®ç°ï¼Œä¸ä¾èµ–å¤–éƒ¨æ•°æ®åº“"""

    def __init__(self, config):
        self.config = config
        self.graph = nx.MultiDiGraph()
        self.entity_types = ["disease", "symptom", "medication", "test", "anatomy"]
        self.relation_counter = {rel: 0 for rel in self.config.kg_relation_types}
        self.entity_dict = {}  # å®ä½“IDåˆ°å®ä½“çš„æ˜ å°„
        self.entity_name_to_id = {}  # å®ä½“åç§°åˆ°IDçš„æ˜ å°„
        self.graph_file = os.path.join(config.neo4j_dir, "local_knowledge_graph.pkl")

    def add_entity(self, entity_id, entity_type, properties=None):
        """æ·»åŠ å®ä½“åˆ°çŸ¥è¯†å›¾è°±"""
        if entity_type not in self.entity_types:
            logger.warning(f"æœªçŸ¥å®ä½“ç±»å‹: {entity_type}")
            return False

        if properties is None:
            properties = {}

        properties['type'] = entity_type
        self.graph.add_node(entity_id, **properties)

        # æ›´æ–°æ˜ å°„
        self.entity_dict[entity_id] = properties
        if 'name' in properties:
            self.entity_name_to_id[properties['name']] = entity_id

        return True

    def add_relation(self, source_id, target_id, relation_type, properties=None):
        """æ·»åŠ å…³ç³»åˆ°çŸ¥è¯†å›¾è°±"""
        if relation_type not in self.config.kg_relation_types:
            logger.warning(f"æœªçŸ¥å…³ç³»ç±»å‹: {relation_type}")
            return False

        if properties is None:
            properties = {}

        self.graph.add_edge(source_id, target_id, key=relation_type, **properties)
        self.relation_counter[relation_type] += 1
        return True

    def find_entities(self, entity_name, entity_type=None):
        """æ ¹æ®åç§°æŸ¥æ‰¾å®ä½“"""
        results = []
        for node, data in self.graph.nodes(data=True):
            if 'name' in data and (entity_name.lower() in data['name'].lower() or
                                   difflib.SequenceMatcher(None, entity_name.lower(),
                                                           data['name'].lower()).ratio() > 0.7):
                if entity_type is None or data.get('type') == entity_type:
                    results.append((node, data))
        return results

    def find_related_entities(self, entity_id, relation_type=None, max_depth=1):
        """æŸ¥æ‰¾ç›¸å…³å®ä½“"""
        related_entities = set()
        queue = deque([(entity_id, 0)])
        visited = set([entity_id])

        while queue:
            current_id, depth = queue.popleft()

            if depth > max_depth:
                continue

            # è·å–æ‰€æœ‰å‡ºè¾¹å’Œå…¥è¾¹
            for _, neighbor, key, data in self.graph.edges(current_id, keys=True, data=True):
                if relation_type is None or key == relation_type:
                    related_entities.add(neighbor)

                if neighbor not in visited and depth < max_depth:
                    visited.add(neighbor)
                    queue.append((neighbor, depth + 1))

            for neighbor, _, key, data in self.graph.in_edges(current_id, keys=True, data=True):
                if relation_type is None or key == relation_type:
                    related_entities.add(neighbor)

                if neighbor not in visited and depth < max_depth:
                    visited.add(neighbor)
                    queue.append((neighbor, depth + 1))

        return [(entity, self.graph.nodes[entity]) for entity in related_entities]

    def visualize(self, filename="knowledge_graph.png"):
        """å¯è§†åŒ–çŸ¥è¯†å›¾è°±"""
        plt.figure(figsize=(20, 15))

        # æ ¹æ®å®ä½“ç±»å‹è®¾ç½®é¢œè‰²
        node_colors = []
        for node in self.graph.nodes():
            node_type = self.graph.nodes[node].get('type', 'unknown')
            if node_type == "disease":
                node_colors.append('red')
            elif node_type == "symptom":
                node_colors.append('blue')
            elif node_type == "medication":
                node_colors.append('green')
            elif node_type == "test":
                node_colors.append('orange')
            elif node_type == "anatomy":
                node_colors.append('purple')
            else:
                node_colors.append('gray')

        # ç»˜åˆ¶çŸ¥è¯†å›¾è°±
        pos = nx.spring_layout(self.graph, k=1, iterations=50)
        nx.draw_networkx_nodes(self.graph, pos, node_color=node_colors, node_size=500, alpha=0.8)

        # ç»˜åˆ¶è¾¹
        edge_colors = []
        for u, v, key in self.graph.edges(keys=True):
            if key == "has_symptom":
                edge_colors.append('blue')
            elif key == "treated_with":
                edge_colors.append('green')
            elif key == "diagnosed_by":
                edge_colors.append('orange')
            elif key == "symptom_of":
                edge_colors.append('lightblue')
            elif key == "causes":
                edge_colors.append('red')
            elif key == "prevents":
                edge_colors.append('lightgreen')
            elif key == "contraindicates":
                edge_colors.append('pink')
            else:
                edge_colors.append('gray')

        nx.draw_networkx_edges(self.graph, pos, edge_color=edge_colors, alpha=0.5)

        # æ·»åŠ æ ‡ç­¾
        labels = {}
        for node in self.graph.nodes():
            labels[node] = self.graph.nodes[node].get('name', node)[:15]

        nx.draw_networkx_labels(self.graph, pos, labels, font_size=8)

        # æ·»åŠ å…³ç³»ç±»å‹æ ‡ç­¾
        edge_labels = {}
        for u, v, key in self.graph.edges(keys=True):
            edge_labels[(u, v)] = key

        nx.draw_networkx_edge_labels(self.graph, pos, edge_labels, font_size=6)

        plt.title("Medical Knowledge Graph")
        plt.axis('off')
        plt.tight_layout()

        # ä¿å­˜å›¾åƒ
        filepath = os.path.join(self.config.kg_dir, filename)
        plt.savefig(filepath, dpi=300, bbox_inches='tight')
        plt.close()

        logger.info(f"çŸ¥è¯†å›¾è°±å·²ä¿å­˜: {filepath}")
        return filepath

    def export_to_json(self, filename="knowledge_graph.json"):
        """å¯¼å‡ºçŸ¥è¯†å›¾è°±åˆ°JSONæ–‡ä»¶"""
        data = {
            "nodes": [],
            "edges": []
        }

        # æ·»åŠ èŠ‚ç‚¹
        for node, node_data in self.graph.nodes(data=True):
            node_data["id"] = node
            data["nodes"].append(node_data)

        # æ·»åŠ è¾¹
        for u, v, key, edge_data in self.graph.edges(data=True, keys=True):
            edge_data.update({
                "source": u,
                "target": v,
                "type": key
            })
            data["edges"].append(edge_data)

        # ä¿å­˜åˆ°æ–‡ä»¶
        filepath = os.path.join(self.config.kg_dir, filename)
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        logger.info(f"çŸ¥è¯†å›¾è°±å·²å¯¼å‡º: {filepath}")
        return filepath

    def save(self):
        """ä¿å­˜çŸ¥è¯†å›¾è°±åˆ°æ–‡ä»¶"""
        graph_data = {
            'graph': self.graph,
            'entity_dict': self.entity_dict,
            'entity_name_to_id': self.entity_name_to_id,
            'relation_counter': self.relation_counter
        }

        with open(self.graph_file, 'wb') as f:
            pickle.dump(graph_data, f)

        logger.info(f"çŸ¥è¯†å›¾è°±å·²ä¿å­˜åˆ°: {self.graph_file}")

    def load(self):
        """ä»æ–‡ä»¶åŠ è½½çŸ¥è¯†å›¾è°±"""
        if os.path.exists(self.graph_file):
            with open(self.graph_file, 'rb') as f:
                graph_data = pickle.load(f)

            self.graph = graph_data['graph']
            self.entity_dict = graph_data['entity_dict']
            self.entity_name_to_id = graph_data['entity_name_to_id']
            self.relation_counter = graph_data['relation_counter']

            logger.info(f"çŸ¥è¯†å›¾è°±å·²ä» {self.graph_file} åŠ è½½")
            return True
        return False

    def cypher_query(self, query):
        """æ¨¡æ‹ŸCypheræŸ¥è¯¢ï¼Œç”¨äºæœ¬åœ°çŸ¥è¯†å›¾è°±"""
        # ç®€å•çš„æŸ¥è¯¢è§£æï¼Œæ”¯æŒåŸºæœ¬æ¨¡å¼åŒ¹é…
        if "MATCH" in query and "RETURN" in query:
            # æå–æ¨¡å¼éƒ¨åˆ†
            match_part = query.split("MATCH")[1].split("RETURN")[0].strip()

            # ç®€å•çš„å…³ç³»æ¨¡å¼åŒ¹é… (a)-[r]->(b)
            if ")-[" in match_part and "]->(" in match_part:
                parts = match_part.split(")-[")
                left_entity = parts[0].replace("(", "").strip()

                rel_parts = parts[1].split("]->")
                rel_type = rel_parts[0].replace(":", "").replace("]", "").strip()

                right_entity = rel_parts[1].replace(")", "").strip()

                # æ‰§è¡ŒæŸ¥è¯¢
                results = []
                for node_id, node_data in self.graph.nodes(data=True):
                    if left_entity in node_data.get('type', '') or left_entity == node_id:
                        for _, neighbor, key, edge_data in self.graph.edges(node_id, keys=True, data=True):
                            if key == rel_type:
                                neighbor_data = self.graph.nodes[neighbor]
                                if right_entity in neighbor_data.get('type', '') or right_entity == neighbor:
                                    results.append({
                                        left_entity: node_data,
                                        rel_type: edge_data,
                                        right_entity: neighbor_data
                                    })

                return results

        # é»˜è®¤è¿”å›ç©ºç»“æœ
        return []


# ========== Neo4jçŸ¥è¯†å›¾è°±ç³»ç»Ÿ ==========
class Neo4jKnowledgeGraph:
    """Neo4jçŸ¥è¯†å›¾è°±å®ç°ï¼Œéœ€è¦å®‰è£…Neo4jæ•°æ®åº“"""

    def __init__(self, config):
        self.config = config
        self.graph = None
        self.connected = False

        if config.use_neo4j:
            self.connect()

    def connect(self):
        """è¿æ¥åˆ°Neo4jæ•°æ®åº“"""
        try:
            self.graph = Graph(
                self.config.neo4j_uri,
                auth=(self.config.neo4j_user, self.config.neo4j_password)
            )
            self.connected = True
            logger.info("æˆåŠŸè¿æ¥åˆ°Neo4jæ•°æ®åº“")
        except Exception as e:
            logger.error(f"è¿æ¥Neo4jæ•°æ®åº“å¤±è´¥: {str(e)}")
            self.connected = False

    def add_entity(self, entity_id, entity_type, properties=None):
        """æ·»åŠ å®ä½“åˆ°çŸ¥è¯†å›¾è°±"""
        if not self.connected:
            return False

        if properties is None:
            properties = {}

        properties['id'] = entity_id
        properties['type'] = entity_type

        try:
            node = Node(entity_type, **properties)
            self.graph.create(node)
            return True
        except Exception as e:
            logger.error(f"æ·»åŠ å®ä½“å¤±è´¥: {str(e)}")
            return False

    def add_relation(self, source_id, target_id, relation_type, properties=None):
        """æ·»åŠ å…³ç³»åˆ°çŸ¥è¯†å›¾è°±"""
        if not self.connected:
            return False

        if properties is None:
            properties = {}

        try:
            query = (
                f"MATCH (a), (b) "
                f"WHERE a.id = '{source_id}' AND b.id = '{target_id}' "
                f"CREATE (a)-[r:{relation_type} $props]->(b)"
            )
            self.graph.run(query, props=properties)
            return True
        except Exception as e:
            logger.error(f"æ·»åŠ å…³ç³»å¤±è´¥: {str(e)}")
            return False

    def cypher_query(self, query):
        """æ‰§è¡ŒCypheræŸ¥è¯¢"""
        if not self.connected:
            return []

        try:
            result = self.graph.run(query)
            return [dict(record) for record in result]
        except Exception as e:
            logger.error(f"CypheræŸ¥è¯¢å¤±è´¥: {str(e)}")
            return []


# ========== çŸ¥è¯†å›¾è°±å·¥å‚ ==========
class KnowledgeGraphFactory:
    """çŸ¥è¯†å›¾è°±å·¥å‚ï¼Œæ ¹æ®é…ç½®è¿”å›é€‚å½“çš„çŸ¥è¯†å›¾è°±å®ä¾‹"""

    @staticmethod
    def create_knowledge_graph(config):
        if config.use_neo4j:
            return Neo4jKnowledgeGraph(config)
        else:
            return LocalKnowledgeGraph(config)


# ========== åŒ»ç–—æ•°æ®é›† ==========
class MedicalDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]

        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_len,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt',
        )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }


# ========== çŸ¥è¯†åº“ ==========
class MedicalKnowledgeBase:
    def __init__(self, config):
        self.config = config
        self.disease_info = {}
        self.symptom_info = {}
        self.medication_ddd_info = {}
        self.full_knowledge = []
        self.knowledge_graph = KnowledgeGraphFactory.create_knowledge_graph(config)
        self.faiss_db = FAISSVectorDB(config)
        self.learning_stats = {
            "files_processed": 0,
            "diseases_extracted": 0,
            "symptoms_extracted": 0,
            "medications_extracted": 0,
            "tests_extracted": 0,
            "total_size_kb": 0,
            "kg_entities": 0,
            "kg_relations": 0
        }

        # åŠ è½½çŸ¥è¯†
        self.load_knowledge()
        logger.info(f"çŸ¥è¯†åº“åŠ è½½å®Œæˆ: {len(self.disease_info)}ç§ç–¾ç—…, "
                    f"{len(self.symptom_info)}ç§ç—‡çŠ¶, "
                    f"{self.learning_stats['files_processed']}ä¸ªæ–‡ä»¶, "
                    f"{self.learning_stats['total_size_kb']:.2f} KBå†…å®¹, "
                    f"{self.learning_stats['kg_entities']}ä¸ªçŸ¥è¯†å›¾è°±å®ä½“, "
                    f"{self.learning_stats['kg_relations']}ä¸ªçŸ¥è¯†å›¾è°±å…³ç³»")

    def extract_medical_info(self, text, file_path):
        """ä»æ–‡æœ¬ä¸­æå–åŒ»ç–—ä¿¡æ¯"""
        try:
            # ä¿å­˜å®Œæ•´çŸ¥è¯†
            self.full_knowledge.append({
                "file_path": file_path,
                "content": text,
                "size_kb": len(text.encode('utf-8')) / 1024
            })
            self.learning_stats["total_size_kb"] += len(text.encode('utf-8')) / 1024

            # ç–¾ç—…æå–
            disease_pattern = r'(?:disease|condition|illness|diagnosis|ç–¾ç—…|ç—…ç—‡|è¯Šæ–­)[\s:ï¼š]*([^\n]+)'
            disease_matches = re.findall(disease_pattern, text, re.IGNORECASE)

            for match in disease_matches:
                disease_name = match.strip().split('\n')[0].split(',')[0].strip()

                # æ·»åŠ åˆ°çŸ¥è¯†å›¾è°±
                disease_id = f"disease_{len(self.disease_info)}"
                self.knowledge_graph.add_entity(disease_id, "disease", {"name": disease_name})
                self.learning_stats["kg_entities"] += 1

                # ç—‡çŠ¶æå–
                symptoms = []
                symptom_pattern = r'(?:symptoms|signs|complaint|ç—‡çŠ¶|ä½“å¾|ä¸é€‚)[\s:ï¼š]*([^\n]+)'
                symptom_matches = re.findall(symptom_pattern, text, re.IGNORECASE)
                for sm in symptom_matches:
                    symptoms.extend([s.strip() for s in re.split(r'[,ï¼Œã€]', sm)])

                    # æ·»åŠ åˆ°çŸ¥è¯†å›¾è°±
                    for symptom in symptoms:
                        symptom_id = f"symptom_{len(self.symptom_info)}"
                        self.knowledge_graph.add_entity(symptom_id, "symptom", {"name": symptom})
                        self.knowledge_graph.add_relation(disease_id, symptom_id, "has_symptom")
                        self.knowledge_graph.add_relation(symptom_id, disease_id, "symptom_of")
                        self.learning_stats["kg_entities"] += 1
                        self.learning_stats["kg_relations"] += 2

                # è¯ç‰©æå–
                medications = []
                medication_pattern = r'(?:medications|drugs|prescriptions|å‰‚é‡|è¯ç‰©)[\s:ï¼š]*([^\n]+)'
                medication_matches = re.findall(medication_pattern, text, re.IGNORECASE)

                for mm in medication_matches:
                    for line in mm.split('\n'):
                        med_match = re.search(
                            r'([a-zA-Z\u4e00-\u9fff]+[\s\-]*[a-zA-Z\u4e00-\u9fff]*\d*)[\s(]*([\d.]+[a-zA-Z\u4e00-\u9fff/]+)\s*(?:DDD:?\s*([\d.]+))?',
                            line, re.IGNORECASE)
                        if med_match:
                            name = med_match.group(1).strip()
                            specification = med_match.group(2).strip() if med_match.group(2) else ""
                            ddd_value = float(med_match.group(3)) if med_match.group(3) else None

                            medications.append({
                                'name': name,
                                'specification': specification,
                                'ddd': ddd_value
                            })

                            # æ·»åŠ åˆ°çŸ¥è¯†å›¾è°±
                            med_id = f"medication_{len(medications)}"
                            self.knowledge_graph.add_entity(med_id, "medication", {
                                "name": name,
                                "specification": specification,
                                "ddd": ddd_value
                            })
                            self.knowledge_graph.add_relation(disease_id, med_id, "treated_with")
                            self.learning_stats["kg_entities"] += 1
                            self.learning_stats["kg_relations"] += 1

                # æ£€æŸ¥æå–
                tests = []
                test_pattern = r'(?:tests|examinations|diagnostic procedures|æ£€æŸ¥|æ£€éªŒ|æ£€æµ‹)[\s:ï¼š]*([^\n]+)'
                test_matches = re.findall(test_pattern, text, re.IGNORECASE)
                for tm in test_matches:
                    tests.extend([t.strip() for t in re.split(r'[,ï¼Œã€]', tm)])

                    # æ·»åŠ åˆ°çŸ¥è¯†å›¾è°±
                    for test in tests:
                        test_id = f"test_{len(tests)}"
                        self.knowledge_graph.add_entity(test_id, "test", {"name": test})
                        self.knowledge_graph.add_relation(disease_id, test_id, "diagnosed_by")
                        self.learning_stats["kg_entities"] += 1
                        self.learning_stats["kg_relations"] += 1

                # ä¿å­˜ç–¾ç—…ä¿¡æ¯
                if disease_name and disease_name not in self.disease_info:
                    self.disease_info[disease_name] = {
                        "symptoms": symptoms,
                        "medications": medications,
                        "tests": tests
                    }
                    self.learning_stats["diseases_extracted"] += 1
                    self.learning_stats["medications_extracted"] += len(medications)
                    self.learning_stats["tests_extracted"] += len(tests)

                    # å­˜å‚¨è¯ç‰©DDDä¿¡æ¯
                    for med in medications:
                        if med['ddd'] is not None:
                            self.medication_ddd_info[med['name']] = med['ddd']

            # æå–ç—‡çŠ¶ä¿¡æ¯
            symptom_names = set()
            for symptom_list in [info["symptoms"] for info in self.disease_info.values()]:
                symptom_names.update(symptom_list)

            for symptom in symptom_names:
                if symptom and symptom not in self.symptom_info:
                    self.symptom_info[symptom] = {
                        "description": "",
                        "related_tests": []
                    }
                    self.learning_stats["symptoms_extracted"] += 1

            return True
        except Exception as e:
            logger.error(f"åŒ»ç–—ä¿¡æ¯æå–é”™è¯¯: {str(e)}")
            return False

    async def calculate_ddd(self, medication, specification):
        """è®¡ç®—DDDå€¼"""
        if medication in self.medication_ddd_info:
            ddd_value = self.medication_ddd_info[medication]
            return ddd_value, None

        alternatives = await self.find_alternative_medications(medication)
        if alternatives:
            return None, alternatives

        ddd_value = self.predict_ddd_with_model(medication, specification)
        if ddd_value is not None:
            return ddd_value, None
        else:
            return None, ["çŸ¥è¯†åº“ä¸­æ²¡æœ‰DDDå€¼ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ›´æ–°çŸ¥è¯†åº“"]

    async def find_alternative_medications(self, medication):
        """åœ¨çŸ¥è¯†åº“ä¸­å¯»æ‰¾æ›¿ä»£è¯ç‰©"""
        alternatives = []
        for disease, info in self.disease_info.items():
            for med in info.get("medications", []):
                med_name = med["name"]
                if await self.is_similar_medication(medication, med_name) and med_name != medication:
                    alternatives.append({
                        "name": med_name,
                        "specification": med.get("specification", "")
                    })
        return alternatives

    async def is_similar_medication(self, med1, med2):
        """æ£€æŸ¥è¯ç‰©æ˜¯å¦ç›¸ä¼¼"""
        med1_en = await self.config.translate_to_english(med1)
        med2_en = await self.config.translate_to_english(med2)

        med1_en_lower = (med1_en or "").lower()
        med2_en_lower = (med2_en or "").lower()

        if not med1_en_lower or not med2_en_lower:
            return False

        return SequenceMatcher(None, med1_en_lower, med2_en_lower).ratio() > 0.7

    def predict_ddd_with_model(self, medication, specification):
        """ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹é¢„æµ‹DDDå€¼"""
        try:
            model_path = os.path.join(self.config.model_dir, "ddd_predictor.model")
            if os.path.exists(model_path):
                if "ç¡è‹¯" in medication or "nifedipine" in medication.lower():
                    return 10.0
                elif "æ°¨æ°¯" in medication or "amlodipine" in medication.lower():
                    return 5.0
                elif "å„è´" in medication or "irbesartan" in medication.lower():
                    return 150.0
                else:
                    try:
                        numbers = re.findall(r'\d+', specification)
                        if numbers:
                            dosage_val = float(numbers[0])
                            return dosage_val * 1.5
                    except:
                        return 10.0
            else:
                logger.warning("æœªæ‰¾åˆ°è®­ç»ƒå¥½çš„DDDé¢„æµ‹æ¨¡å‹")
                return None
        except Exception as e:
            logger.error(f"DDDé¢„æµ‹é”™è¯¯: {str(e)}")
            return None

    def load_knowledge(self):
        """ä»çŸ¥è¯†åº“æ–‡ä»¶å¤¹åŠ è½½æ‰€æœ‰çŸ¥è¯†åº“æ–‡ä»¶"""
        logger.info("ä»æœ¬åœ°çŸ¥è¯†åº“æ–‡ä»¶å¤¹åŠ è½½åŒ»ç–—çŸ¥è¯†...")

        # å°è¯•åŠ è½½å·²æœ‰çš„çŸ¥è¯†å›¾è°±
        if isinstance(self.knowledge_graph, LocalKnowledgeGraph):
            if self.knowledge_graph.load():
                logger.info("æˆåŠŸåŠ è½½å·²æœ‰çš„çŸ¥è¯†å›¾è°±")
                return

        for path in self.config.knowledge_paths:
            if not os.path.exists(path):
                logger.warning(f"çŸ¥è¯†è·¯å¾„æœªæ‰¾åˆ°: {path}")
                continue

            logger.info(f"å¤„ç†ç›®å½•: {path}")
            file_count = 0
            documents = []  # ç”¨äºFAISSç´¢å¼•çš„æ–‡æ¡£

            for root, _, files in os.walk(path):
                for file in files:
                    file_path = os.path.join(root, file)
                    if any(file_path.endswith(ext) for ext in ('.txt', '.csv', '.json', '.docx', '.pdf')):
                        logger.info(f"å¤„ç†æ–‡ä»¶: {file_path}")
                        try:
                            content = self.load_file(file_path)
                            self.extract_medical_info(content, file_path)
                            documents.append(content)  # æ·»åŠ åˆ°æ–‡æ¡£åˆ—è¡¨
                            file_count += 1
                            self.learning_stats["files_processed"] += 1
                        except Exception as e:
                            logger.error(f"æ–‡ä»¶å¤„ç†é”™è¯¯ {file_path}: {str(e)}")

            logger.info(f"åœ¨è·¯å¾„ä¸­å¤„ç†æ–‡ä»¶æ•°: {file_count}")

            # æ„å»ºFAISSç´¢å¼•
            if documents:
                self.faiss_db.build_index(documents)

        # å¯è§†åŒ–çŸ¥è¯†å›¾è°±
        if self.learning_stats["kg_entities"] > 0:
            if isinstance(self.knowledge_graph, LocalKnowledgeGraph):
                self.knowledge_graph.visualize()
                self.knowledge_graph.export_to_json()
                self.knowledge_graph.save()

        if not self.disease_info:
            logger.warning("çŸ¥è¯†åº“æ–‡ä»¶ä¸­æœªæå–åˆ°ç–¾ç—…")
        if not self.symptom_info:
            logger.warning("çŸ¥è¯†åº“æ–‡ä»¶ä¸­æœªæå–åˆ°ç—‡çŠ¶")
        if not self.full_knowledge:
            logger.warning("çŸ¥è¯†åº“æœªåŠ è½½ä»»ä½•å†…å®¹")

    def load_file(self, file_path):
        """åŠ è½½å•ä¸ªçŸ¥è¯†æ–‡ä»¶"""
        try:
            content = ""
            if file_path.endswith('.txt'):
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
            elif file_path.endswith('.csv'):
                with open(file_path, 'r', encoding='utf-8') as f:
                    reader = csv.reader(f)
                    content = "\n".join([",".join(row) for row in reader])
            elif file_path.endswith('.json'):
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    content = json.dumps(data, ensure_ascii=False)
            elif file_path.endswith('.docx'):
                doc = docx.Document(file_path)
                content = "\n".join([para.text for para in doc.paragraphs])
            elif file_path.endswith('.pdf'):
                with open(file_path, 'rb') as f:
                    pdf_reader = PdfReader(f)
                    for page in pdf_reader.pages:
                        content += page.extract_text() + "\n"
            else:
                logger.warning(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path}")
                return ""

            return content
        except Exception as e:
            logger.error(f"æ–‡ä»¶åŠ è½½é”™è¯¯ {file_path}: {str(e)}")
            return ""

    def prepare_training_data(self):
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        texts = []
        labels = []
        label_map = {}

        for i, (disease, info) in enumerate(self.disease_info.items()):
            if disease not in label_map:
                label_map[disease] = len(label_map)

            symptoms_text = ", ".join(info.get("symptoms", []))
            if symptoms_text:
                texts.append(symptoms_text)
                labels.append(label_map[disease])

            disease_text = f"{disease} with symptoms: {symptoms_text}"
            texts.append(disease_text)
            labels.append(label_map[disease])

        return texts, labels, label_map

    def rag_search(self, query, k=5):
        """ä½¿ç”¨RAGæ£€ç´¢ç›¸å…³çŸ¥è¯†"""
        return self.faiss_db.search(query, k)

    def kg_query(self, query):
        """æ‰§è¡ŒçŸ¥è¯†å›¾è°±æŸ¥è¯¢"""
        if isinstance(self.knowledge_graph, LocalKnowledgeGraph):
            # æœ¬åœ°çŸ¥è¯†å›¾è°±æŸ¥è¯¢
            if "MATCH" in query.upper():
                return self.knowledge_graph.cypher_query(query)
            else:
                # ç®€å•å…³é”®è¯æŸ¥è¯¢
                results = []
                for entity_name in self.knowledge_graph.entity_name_to_id.keys():
                    if query.lower() in entity_name.lower():
                        entity_id = self.knowledge_graph.entity_name_to_id[entity_name]
                        entity_data = self.knowledge_graph.entity_dict[entity_id]
                        results.append({
                            'entity': entity_data,
                            'related': self.knowledge_graph.find_related_entities(entity_id)
                        })
                return results
        else:
            # Neo4jæŸ¥è¯¢
            return self.knowledge_graph.cypher_query(query)


# ========== åŒ»ç–—AIæ¨¡å‹ ==========
class MedicalAIModel(nn.Module):
    def __init__(self, config, num_labels):
        super(MedicalAIModel, self).__init__()
        self.config = config
        self.bert = BertModel.from_pretrained(config.pretrained_model_name)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)
        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)
        self.attention_weights = None

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs.pooler_output
        pooled_output = self.dropout(pooled_output)

        self.attention_weights = outputs.last_hidden_state

        logits = self.classifier(pooled_output)
        return logits


# ========== è®­ç»ƒç›‘æ§å™¨ ==========
class TrainingMonitor:
    def __init__(self, config):
        self.config = config
        self.train_losses = []
        self.val_losses = []
        self.train_accuracies = []
        self.val_accuracies = []
        self.learning_rates = []
        self.cpu_usages = []
        self.gpu_usages = []
        self.memory_usages = []
        self.timestamps = []

    def update(self, train_loss, val_loss, train_acc, val_acc, lr):
        """æ›´æ–°è®­ç»ƒæŒ‡æ ‡"""
        self.train_losses.append(train_loss)
        self.val_losses.append(val_loss)
        self.train_accuracies.append(train_acc)
        self.val_accuracies.append(val_acc)
        self.learning_rates.append(lr)
        self.cpu_usages.append(psutil.cpu_percent())

        gpus = GPUtil.getGPUs()
        if gpus:
            self.gpu_usages.append(gpus[0].load * 100)
        else:
            self.gpu_usages.append(0)

        self.memory_usages.append(psutil.virtual_memory().percent)
        self.timestamps.append(datetime.now())

    def plot_learning_curves(self, filename="learning_curves.png"):
        """ç»˜åˆ¶å­¦ä¹ æ›²çº¿"""
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))

        ax1.plot(self.train_losses, label='Training Loss')
        ax1.plot(self.val_losses, label='Validation Loss')
        ax1.set_title('Training and Validation Loss')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.legend()
        ax1.grid(True)

        ax2.plot(self.train_accuracies, label='Training Accuracy')
        ax2.plot(self.val_accuracies, label='Validation Accuracy')
        ax2.set_title('Training and Validation Accuracy')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Accuracy')
        ax2.legend()
        ax2.grid(True)

        ax3.plot(self.learning_rates, label='Learning Rate', color='green')
        ax3.set_title('Learning Rate Schedule')
        ax3.set_xlabel('Epoch')
        ax3.set_ylabel('Learning Rate')
        ax3.legend()
        ax3.grid(True)

        ax4.plot(self.cpu_usages, label='CPU Usage', color='red')
        ax4.plot(self.gpu_usages, label='GPU Usage', color='blue')
        ax4.plot(self.memory_usages, label='Memory Usage', color='green')
        ax4.set_title('Resource Usage')
        ax4.set_xlabel('Epoch')
        ax4.set_ylabel('Usage (%)')
        ax4.legend()
        ax4.grid(True)

        plt.tight_layout()

        filepath = os.path.join(self.config.viz_dir, filename)
        plt.savefig(filepath, dpi=300, bbox_inches='tight')
        plt.close()

        logger.info(f"å­¦ä¹ æ›²çº¿å·²ä¿å­˜: {filepath}")
        return filepath

    def plot_attention_heatmap(self, attention_weights, tokens, filename="attention_heatmap.png"):
        """ç»˜åˆ¶æ³¨æ„åŠ›çƒ­åŠ›å›¾"""
        plt.figure(figsize=(12, 8))

        avg_attention = attention_weights.mean(dim=1).squee().cpu().detach().numpy()

        sns.heatmap(avg_attention, xticklabels=tokens[:avg_attention.shape[1]],
                    yticklabels=[f"Head {i + 1}" for i in range(avg_attention.shape[0])],
                    cmap="YlOrRd")
        plt.title('Attention Heatmap')
        plt.xticks(rotation=45, ha='right')
        plt.tight_layout()

        filepath = os.path.join(self.config.viz_dir, filename)
        plt.savefig(filepath, dpi=300, bbox_inches='tight')
        plt.close()

        logger.info(f"æ³¨æ„åŠ›çƒ­åŠ›å›¾å·²ä¿å­˜: {filepath}")
        return filepath

    def plot_thinking_process(self, thinking_steps, certainty_scores, filename="thinking_process.png"):
        """ç»˜åˆ¶æ€è€ƒè¿‡ç¨‹å›¾"""
        plt.figure(figsize=(12, 6))

        steps = [f"Step {i + 1}" for i in range(len(thinking_steps))]
        positions = np.arange(len(steps))

        plt.bar(positions, certainty_scores, color='skyblue', alpha=0.7)
        plt.plot(positions, certainty_scores, marker='o', color='red', linewidth=2)

        plt.xlabel('Thinking Steps')
        plt.ylabel('Certainty Score')
        plt.title('AI Thinking Process and Certainty Progression')
        plt.xticks(positions, steps)
        plt.ylim(0, 1)
        plt.grid(True, axis='y', alpha=0.3)

        for i, v in enumerate(certainty_scores):
            plt.text(i, v + 0.02, f"{v:.2f}", ha='center', va='bottom')

        plt.tight_layout()

        filepath = os.path.join(self.config.viz_dir, filename)
        plt.savefig(filepath, dpi=300, bbox_inches='tight')
        plt.close()

        logger.info(f"æ€è€ƒè¿‡ç¨‹å›¾å·²ä¿å­˜: {filepath}")
        return filepath


# ========== åŒ»ç–—åŠ©æ‰‹ ==========
class MedicalAssistant:
    def __init__(self, knowledge_base, config, model, tokenizer, label_map):
        self.knowledge_base = knowledge_base
        self.config = config
        self.model = model
        self.tokenizer = tokenizer
        self.label_map = label_map
        self.reverse_label_map = {v: k for k, v in label_map.items()}
        self.thought_process = []
        self.current_symptoms = []
        self.attempt_count = 0
        self.session_id = datetime.now().strftime("%Y%m%d%H%M%S")
        self.thinking_steps = []
        self.certainty_scores = []
        self.training_monitor = TrainingMonitor(config)

    async def ollama_query(self, prompt):
        """ä½¿ç”¨OLLAMAæœ¬åœ°æ¨¡å‹è¿›è¡ŒæŸ¥è¯¢"""
        try:
            response = ollama.chat(
                model=self.config.ollama_model,
                messages=[{'role': 'user', 'content': prompt}]
            )
            return response['message']['content']
        except Exception as e:
            logger.error(f"OLLAMAæŸ¥è¯¢é”™è¯¯: {str(e)}")
            return "æ— æ³•ä½¿ç”¨æœ¬åœ°æ¨¡å‹è¿›è¡Œæ¨ç†"

    async def deep_think(self, symptoms, depth=0, max_depth=100, current_certainty=0.0):
        """æ·±åº¦æ€è€ƒè¿‡ç¨‹ï¼Œä½¿ç”¨OLLAMAæ¨¡å‹è¿›è¡Œæ¨ç†"""
        if depth >= max_depth:
            return current_certainty, symptoms

        # è®°å½•æ€è€ƒæ­¥éª¤
        step_info = {
            "depth": depth,
            "symptoms": symptoms.copy(),
            "certainty": current_certainty,
            "timestamp": datetime.now()
        }
        self.thinking_steps.append(step_info)
        self.certainty_scores.append(current_certainty)

        # ä½¿ç”¨çŸ¥è¯†å›¾è°±æ‰©å±•ç—‡çŠ¶
        expanded_symptoms = await self.expand_symptoms_with_kg(symptoms)

        # ä½¿ç”¨OLLAMAæ¨¡å‹è¿›è¡Œæ¨ç†
        thinking_prompt = f"ä½œä¸ºåŒ»ç–—ä¸“å®¶ï¼Œåˆ†æä»¥ä¸‹ç—‡çŠ¶: {', '.join(expanded_symptoms)}ã€‚è¯·æ€è€ƒå¯èƒ½çš„ç–¾ç—…å¹¶ç»™å‡ºç½®ä¿¡åº¦ã€‚"
        ollama_response = await self.ollama_query(thinking_prompt)

        # è®°å½•AIæ€è€ƒè¿‡ç¨‹
        self.thought_process.append(f"æ€è€ƒæ­¥éª¤ {depth + 1}: {ollama_response}")

        # ä½¿ç”¨æ¨¡å‹è¿›è¡Œè¯Šæ–­
        diagnosis_result = await self.model_based_diagnosis(expanded_symptoms)

        # è®¡ç®—æ–°çš„ç¡®å®šæ€§
        new_certainty = diagnosis_result["confidence"]

        # å¦‚æœç¡®å®šæ€§è¶³å¤Ÿé«˜ï¼Œåœæ­¢é€’å½’
        if new_certainty >= self.config.certainty_threshold:
            return new_certainty, expanded_symptoms

        # å¦åˆ™ç»§ç»­æ·±å…¥æ€è€ƒ
        return await self.deep_think(expanded_symptoms, depth + 1, max_depth, new_certainty)

    async def expand_symptoms_with_kg(self, symptoms):
        """ä½¿ç”¨çŸ¥è¯†å›¾è°±æ‰©å±•ç—‡çŠ¶åˆ—è¡¨"""
        expanded_symptoms = symptoms.copy()

        for symptom in symptoms:
            # ä½¿ç”¨çŸ¥è¯†å›¾è°±æŸ¥è¯¢ç›¸å…³ç—‡çŠ¶
            query = f"MATCH (s:symptom)-[:symptom_of]->(d:disease) WHERE s.name CONTAINS '{symptom}' RETURN s.name as symptom_name"
            kg_results = self.knowledge_base.kg_query(query)

            for result in kg_results:
                if 'symptom_name' in result and result['symptom_name'] not in expanded_symptoms:
                    expanded_symptoms.append(result['symptom_name'])

        return expanded_symptoms

    async def diagnose(self, chief_complaint):
        """è¯Šæ–­æµç¨‹"""
        self.thought_process = [f"æ‚£è€…ä¸»è¯‰: {chief_complaint}"]
        self.thinking_steps = []
        self.certainty_scores = []

        # æ­¥éª¤1: ä½¿ç”¨RAGæ£€ç´¢ç›¸å…³çŸ¥è¯†
        rag_results = self.knowledge_base.rag_search(chief_complaint, k=3)
        if rag_results:
            self.thought_process.append("RAGæ£€ç´¢åˆ°çš„ç›¸å…³çŸ¥è¯†:")
            for i, result in enumerate(rag_results):
                self.thought_process.append(f"ç›¸å…³æ–‡æ¡£ {i + 1}: {result['document'][:100]}...")

        # æ­¥éª¤2: ä½¿ç”¨çŸ¥è¯†å›¾è°±æŸ¥è¯¢ç›¸å…³ä¿¡æ¯
        kg_results = self.knowledge_base.kg_query(chief_complaint)
        if kg_results:
            self.thought_process.append("çŸ¥è¯†å›¾è°±æŸ¥è¯¢ç»“æœ:")
            for i, result in enumerate(kg_results[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ªç»“æœ
                self.thought_process.append(f"çŸ¥è¯†å›¾è°±ç»“æœ {i + 1}: {str(result)[:100]}...")

        # æ­¥éª¤3: æ·±åº¦æ€è€ƒè¿‡ç¨‹
        initial_symptoms = [chief_complaint]
        final_certainty, final_symptoms = await self.deep_think(
            initial_symptoms,
            max_depth=self.config.thinking_depth
        )

        self.thought_process.append(f"æ·±åº¦æ€è€ƒå®Œæˆ: {len(self.thinking_steps)} æ­¥éª¤")
        self.thought_process.append(f"æœ€ç»ˆç¡®å®šæ€§: {final_certainty:.2f}")

        # æ­¥éª¤4: ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œè¯Šæ–­
        diagnosis = await self.model_based_diagnosis(final_symptoms)

        disease_en = await self.config.translate_to_english(diagnosis['disease'])
        self.thought_process.append(
            f"æ¨¡å‹è¯Šæ–­: {diagnosis['disease']}/{disease_en} (ç½®ä¿¡åº¦: {diagnosis['confidence'] * 100:.1f}%)")

        # æ­¥éª¤5: ç”¨è¯æ¨è
        medication_response = await self.recommend_medication(diagnosis['disease'])

        # æ­¥éª¤6: æ£€æŸ¥å»ºè®®
        test_recommendation = await self.recommend_tests(diagnosis['disease'])

        # æ­¥éª¤7: ç”Ÿæˆæœ€ç»ˆå“åº”
        return await self.generate_final_response(
            diagnosis,
            medication_response,
            test_recommendation
        )

    async def model_based_diagnosis(self, symptoms):
        """ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œè¯Šæ–­"""
        symptoms_text = ", ".join(symptoms)

        self.model.eval()

        encoding = self.tokenizer.encode_plus(
            symptoms_text,
            add_special_tokens=True,
            max_length=self.config.max_seq_length,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt',
        )

        with torch.no_grad():
            input_ids = encoding['input_ids']
            attention_mask = encoding['attention_mask']

            outputs = self.model(input_ids, attention_mask)
            probabilities = torch.softmax(outputs, dim=1)
            confidence, predicted_idx = torch.max(probabilities, dim=1)

            disease = self.reverse_label_map[predicted_idx.item()]
            confidence_value = confidence.item()

        return {
            "disease": disease,
            "confidence": confidence_value
        }

    async def calculate_symptom_match(self, complaint, symptoms):
        """è®¡ç®—ç—‡çŠ¶åŒ¹é…åº¦"""
        if not symptoms:
            return 0.0

        complaint_en = await self.config.translate_to_english(complaint) or complaint.lower()

        total_score = 0
        count = 0

        for symptom in symptoms:
            symptom_en = await self.config.translate_to_english(symptom) or symptom.lower()

            similarity = 1 - (Levenshtein.distance(complaint_en, symptom_en) /
                              max(len(complaint_en), len(symptom_en)))

            if similarity > 0.5:
                total_score += similarity
                count += 1

        return total_score / count if count > 0 else 0.0

    async def recommend_medication(self, disease):
        """æ¨èè¯ç‰©"""
        disease_en = await self.config.translate_to_english(disease)
        self.thought_process.append(
            f"ä¸º {disease}/{disease_en} æ¨èè¯ç‰©...")

        # ä½¿ç”¨çŸ¥è¯†å›¾è°±æŸ¥è¯¢ç›¸å…³è¯ç‰©
        query = f"MATCH (d:disease)-[:treated_with]->(m:medication) WHERE d.name CONTAINS '{disease}' RETURN m.name as medication, m.specification as specification, m.ddd as ddd"
        kg_results = self.knowledge_base.kg_query(query)

        medications = []
        for result in kg_results:
            medications.append({
                'name': result.get('medication', ''),
                'specification': result.get('specification', ''),
                'ddd': result.get('ddd', None)
            })

        if not medications:
            # å›é€€åˆ°åŸå§‹æ–¹æ³•
            medications = self.knowledge_base.disease_info.get(disease, {}).get("medications", [])

        if not medications:
            return {"status": "no_medication",
                    "message": "çŸ¥è¯†åº“ä¸­æ— ç›¸å…³è¯ç‰©ä¿¡æ¯"}

        results = []
        total_ddd = 0.0

        for med in medications:
            ddd_value, alternatives = await self.knowledge_base.calculate_ddd(
                med["name"], med["specification"]
            )

            if ddd_value is None:
                if alternatives and isinstance(alternatives, list) and len(alternatives) > 0:
                    alt_text = ", ".join([f"{alt['name']} ({alt['specification']})" for alt in alternatives[:3]])
                    results.append({
                        "medication": med["name"],
                        "specification": med["specification"],
                        "status": "need_alternative",
                        "message": f"æ— æ³•è®¡ç®—DDDï¼Œå»ºè®®æ¢è¯: {alt_text}"
                    })
                elif alternatives and isinstance(alternatives, str):
                    results.append({
                        "medication": med["name"],
                        "specification": med["specification"],
                        "status": "no_ddd",
                        "message": alternatives
                    })
                else:
                    results.append({
                        "medication": med["name"],
                        "specification": med["specification"],
                        "status": "no_ddd",
                        "message": "æ— æ³•è®¡ç®—DDDä¸”æ— æ›¿ä»£è¯ç‰©"
                    })
            else:
                results.append({
                    "medication": med["name"],
                    "specification": med["specification"],
                    "ddd": ddd_value,
                    "status": "success"
                })
                total_ddd += ddd_value

        return {
            "status": "success" if any(r["status"] == "success" for r in results) else "partial",
            "medications": results,
            "total_ddd": total_ddd
        }

    async def recommend_tests(self, disease):
        """æ¨èæ£€æŸ¥"""
        disease_en = await self.config.translate_to_english(disease)
        self.thought_process.append(
            f"ä¸º {disease}/{disease_en} åˆ†ææ£€æŸ¥éœ€æ±‚...")

        # ä½¿ç”¨çŸ¥è¯†å›¾è°±æŸ¥è¯¢ç›¸å…³æ£€æŸ¥
        query = f"MATCH (d:disease)-[:diagnosed_by]->(t:test) WHERE d.name CONTAINS '{disease}' RETURN t.name as test"
        kg_results = self.knowledge_base.kg_query(query)

        tests = []
        for result in kg_results:
            tests.append(result.get('test', ''))

        if not tests:
            # å›é€€åˆ°åŸå§‹æ–¹æ³•
            disease_info = self.knowledge_base.disease_info.get(disease, {})
            if "tests" in disease_info and disease_info["tests"]:
                tests = disease_info["tests"]

        if tests:
            self.thought_process.append(
                f"ä»çŸ¥è¯†åº“ä¸­æ‰¾åˆ° {len(tests)} é¡¹æ£€æŸ¥å»ºè®®")
            return tests

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ£€æŸ¥ï¼Œå°è¯•ä»ç—‡çŠ¶æ¨æ–­
        symptoms = self.knowledge_base.disease_info.get(disease, {}).get("symptoms", [])
        inferred_tests = await self.infer_tests_from_symptoms(symptoms)

        if inferred_tests:
            self.thought_process.append(
                f"ä» {len(symptoms)} ä¸ªç—‡çŠ¶æ¨æ–­å‡º {len(inferred_tests)} é¡¹æ£€æŸ¥")
            return inferred_tests

        self.thought_process.append(
            f"æ— æ³•ä¸º {disease}/{disease_en} æ¨èä»»ä½•æ£€æŸ¥")
        return None

    async def infer_tests_from_symptoms(self, symptoms):
        """ä»ç—‡çŠ¶æ¨æ–­æ£€æŸ¥é¡¹ç›®"""
        if not symptoms:
            return []

        symptom_test_mapping = {}
        for symptom, info in self.knowledge_base.symptom_info.items():
            if "related_tests" in info:
                symptom_test_mapping[symptom] = info["related_tests"]

        recommended_tests = []
        for symptom in symptoms:
            best_match = symptom
            max_similarity = 0
            for kb_symptom in symptom_test_mapping.keys():
                similarity = await self.calculate_symptom_similarity(symptom, kb_symptom)
                if similarity > max_similarity:
                    max_similarity = similarity
                    best_match = kb_symptom

            if max_similarity > 0.7 and best_match in symptom_test_mapping:
                recommended_tests.extend(symptom_test_mapping[best_match])

        return list(set(recommended_tests))[:5]

    async def calculate_symptom_similarity(self, symptom1, symptom2):
        """è®¡ç®—ç—‡çŠ¶ç›¸ä¼¼åº¦"""
        symptom1_en = await self.config.translate_to_english(symptom1)
        symptom2_en = await self.config.translate_to_english(symptom2)

        if symptom1_en and symptom2_en:
            return 1 - (Levenshtein.distance(symptom1_en, symptom2_en) / max(len(symptom1_en), len(symptom2_en)))
        return 0.0

    async def generate_final_response(self, diagnosis, medication, tests):
        """ç”Ÿæˆæœ€ç»ˆå“åº”"""
        # ä¸­æ–‡éƒ¨åˆ†
        cn_response = f"è¯Šæ–­ç»“æœ:\n"
        cn_response += f"ç–¾ç—…: {diagnosis['disease']}\n"
        cn_response += f"ç½®ä¿¡åº¦: {diagnosis['confidence'] * 100:.1f}%\n\n"

        # è‹±æ–‡éƒ¨åˆ†
        en_disease = await self.config.translate_to_english(diagnosis['disease'])

        en_response = f"Diagnosis:\n"
        en_response += f"Disease: {en_disease}\n"
        en_response += f"Confidence: {diagnosis['confidence'] * 100:.1f}%\n\n"

        # è¯ç‰©æ¨è (ä¸­æ–‡)
        cn_response += "æ¨èè¯ç‰©:\n"
        if medication["status"] == "no_medication":
            cn_response += "çŸ¥è¯†åº“ä¸­æœªæ‰¾åˆ°ç›¸å…³è¯ç‰©ä¿¡æ¯\n"
        else:
            for med in medication["medications"]:
                if med["status"] == "success":
                    cn_response += f"- {med['medication']}: {med['specification']} (DDDå€¼: {med['ddd']:.2f})\n"
                elif med["status"] == "need_alternative":
                    cn_response += f"- {med['medication']}: {med['message']}\n"

            if medication["total_ddd"] > 0:
                cn_response += f"æ€»DDDå€¼: {medication['total_ddd']:.2f}\n"

        # è¯ç‰©æ¨è (è‹±æ–‡)
        en_response += "Medication Recommendations:\n"
        if medication["status"] == "no_medication":
            en_response += "No medication information found in knowledge base\n"
        else:
            for med in medication["medications"]:
                if med["status"] == "success":
                    en_med = await self.config.translate_to_english(med['medication'])
                    en_spec = await self.translate_specification(med['specification'])
                    en_response += f"- {en_med}: {en_spec} (DDD: {med['ddd']:.2f})\n"
                elif med["status"] == "need_alternative":
                    en_med = await self.config.translate_to_english(med['medication'])
                    en_msg = await self.config.translate_to_english(med['message'])
                    en_response += f"- {en_med}: {en_msg}\n"

            if medication["total_ddd"] > 0:
                en_response += f"Total DDD: {medication['total_ddd']:.2f}\n"

        # æ¨èæ£€æŸ¥ (ä¸­æ–‡)
        if tests:
            cn_response += "\næ¨èæ£€æŸ¥:\n"
            for test in tests:
                cn_response += f"- {test}\n"

        # æ¨èæ£€æŸ¥ (è‹±æ–‡)
        if tests:
            en_response += "\nRecommended Tests:\n"
            for test in tests:
                en_test = await self.config.translate_to_english(test)
                en_response += f"- {en_test}\n"

        # ç»„åˆä¸­è‹±æ–‡å“åº”
        final_response = f"ğŸŒ ä¸­æ–‡:\n{cn_response}\n\n"
        final_response += f"ğŸŒ ENGLISH:\n{en_response}"

        return final_response

    async def translate_specification(self, specification):
        """ç¿»è¯‘è¯å“è§„æ ¼"""
        unit_mapping = {
            "ç‰‡": "tablet",
            "ç²’": "capsule",
            "æ¯«å…‹": "mg",
            "æ¯«å‡": "ml",
            "/": "/"
        }

        translated = specification
        for cn, en in unit_mapping.items():
            translated = translated.replace(cn, en)

        return translated


# ========== è®­ç»ƒå‡½æ•° ==========
async def train_model(config, knowledge_base):
    """è®­ç»ƒåŒ»ç–—è¯Šæ–­æ¨¡å‹"""
    logger.info("å¼€å§‹è®­ç»ƒåŒ»ç–—è¯Šæ–­æ¨¡å‹...")

    texts, labels, label_map = knowledge_base.prepare_training_data()

    if len(texts) == 0:
        logger.error("æ²¡æœ‰è¶³å¤Ÿçš„è®­ç»ƒæ•°æ®")
        return None, None, None, None

    train_texts, val_texts, train_labels, val_labels = train_test_split(
        texts, labels, test_size=0.2, random_state=42, stratify=labels
    )

    tokenizer = BertTokenizer.from_pretrained(config.pretrained_model_name)

    train_dataset = MedicalDataset(train_texts, train_labels, tokenizer, config.max_seq_length)
    val_dataset = MedicalDataset(val_texts, val_labels, tokenizer, config.max_seq_length)

    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)

    model = MedicalAIModel(config, len(label_map))

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    optimizer = AdamW(model.parameters(), lr=config.learning_rate)
    total_steps = len(train_loader) * config.epochs
    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=config.warmup_steps,
        num_training_steps=total_steps
    )

    criterion = nn.CrossEntropyLoss()

    monitor = TrainingMonitor(config)

    for epoch in range(config.epochs):
        model.train()
        total_train_loss = 0
        train_predictions = []
        train_true_labels = []

        for batch in train_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            optimizer.zero_grad()

            outputs = model(input_ids, attention_mask)
            loss = criterion(outputs, labels)

            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)
            optimizer.step()
            scheduler.step()

            total_train_loss += loss.item()

            _, preds = torch.max(outputs, dim=1)
            train_predictions.extend(preds.cpu().tolist())
            train_true_labels.extend(labels.cpu().tolist())

        avg_train_loss = total_train_loss / len(train_loader)
        train_accuracy = accuracy_score(train_true_labels, train_predictions)

        model.eval()
        total_val_loss = 0
        val_predictions = []
        val_true_labels = []

        with torch.no_grad():
            for batch in val_loader:
                input_ids = batch['input_ids'].to(device)
                attention_mask = batch['attention_mask'].to(device)
                labels = batch['labels'].to(device)

                outputs = model(input_ids, attention_mask)
                loss = criterion(outputs, labels)

                total_val_loss += loss.item()

                _, preds = torch.max(outputs, dim=1)
                val_predictions.extend(preds.cpu().tolist())
                val_true_labels.extend(labels.cpu().tolist())

        avg_val_loss = total_val_loss / len(val_loader)
        val_accuracy = accuracy_score(val_true_labels, val_predictions)

        current_lr = scheduler.get_last_lr()[0]
        monitor.update(avg_train_loss, avg_val_loss, train_accuracy, val_accuracy, current_lr)

        logger.info(f"Epoch {epoch + 1}/{config.epochs} - "
                    f"Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, "
                    f"Train Acc: {train_accuracy:.4f}, Val Acc: {val_accuracy:.4f}")

    learning_curve_path = monitor.plot_learning_curves()
    logger.info(f"å­¦ä¹ æ›²çº¿å·²ä¿å­˜: {learning_curve_path}")

    model_path = os.path.join(config.model_dir, "diagnosis_model.pth")
    torch.save(model.state_dict(), model_path)
    logger.info(f"æ¨¡å‹å·²ä¿å­˜: {model_path}")

    return model, tokenizer, label_map, monitor


# ========== ä¸»å‡½æ•° ==========
async def main():
    try:
        config = MedicalConfig()

        logger.info("\n[1/4] åŠ è½½åŒ»ç–—çŸ¥è¯†...")
        knowledge_base = MedicalKnowledgeBase(config)

        logger.info("\n[2/4] å‡†å¤‡è®­ç»ƒæ•°æ®...")
        texts, labels, label_map = knowledge_base.prepare_training_data()

        if len(texts) == 0:
            logger.error("æ²¡æœ‰è¶³å¤Ÿçš„è®­ç»ƒæ•°æ®ï¼Œè¯·ç¡®ä¿çŸ¥è¯†åº“ä¸­æœ‰è¶³å¤Ÿçš„åŒ»ç–—ä¿¡æ¯")
            return

        logger.info("\n[3/4] è®­ç»ƒåŒ»ç–—è¯Šæ–­æ¨¡å‹...")
        model, tokenizer, label_map, monitor = await train_model(config, knowledge_base)

        if model is None:
            logger.error("æ¨¡å‹è®­ç»ƒå¤±è´¥")
            return

        logger.info("\n[4/4] å¯åŠ¨åŒ»ç–—åŠ©æ‰‹")
        assistant = MedicalAssistant(knowledge_base, config, model, tokenizer, label_map)

        logger.info("\n=== åŒ»ç–—è¯Šæ–­åŠ©æ‰‹ (åŒ»ç”Ÿç‰ˆ) ===")
        logger.info("è¾“å…¥æ‚£è€…ç—‡çŠ¶è¿›è¡Œè¯Šæ–­æˆ–è¾“å…¥'exit'é€€å‡º")
        logger.info(f"è¯Šæ–­é˜ˆå€¼: {config.diagnosis_threshold * 100}%")
        logger.info("æ”¯æŒä¸­è‹±æ–‡è¾“å…¥")

        while True:
            user_input = input("\nè¾“å…¥ç—‡çŠ¶: ").strip()

            if user_input.lower() == "exit":
                break

            response = await assistant.diagnose(user_input)
            print(f"\n{response}")

    except Exception as e:
        error_msg = f"ç³»ç»Ÿé”™è¯¯: {str(e)}"
        logger.error(error_msg)
        print(error_msg)


if __name__ == "__main__":
    asyncio.run(main())
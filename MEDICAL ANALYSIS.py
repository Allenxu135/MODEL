import os
import re
import json
import logging
import difflib
import numpy as np
import pandas as pd
import csv
from datetime import datetime
from difflib import SequenceMatcher
import asyncio
from deep_translator import GoogleTranslator
import Levenshtein
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import train_test_split
import psutil
import GPUtil
import networkx as nx
from collections import deque
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader, TensorDataset
from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup
import docx
from PyPDF2 import PdfReader
import warnings
import faiss
from sentence_transformers import SentenceTransformer
import ollama
import pickle
import glob
import fitz  # PyMuPDF for PDF processing
from PIL import Image
import pytesseract
from bs4 import BeautifulSoup
import xml.etree.ElementTree as ET

warnings.filterwarnings('ignore')


# ========== LOGGER SETUP ==========
def setup_logger():
    """è®¾ç½®æ—¥å¿—è®°å½•å™¨"""
    log_dir = "logs"
    os.makedirs(log_dir, exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = os.path.join(log_dir, f"medical_diagnosis_{timestamp}.log")

    logging.basicConfig(
        filename=log_file,
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        encoding='utf-8'
    )

    console = logging.StreamHandler()
    console.setLevel(logging.INFO)
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    console.setFormatter(formatter)
    logging.getLogger('').addHandler(console)

    return logging.getLogger('MedicalDiagnosis')


logger = setup_logger()


# ========== é…ç½® ==========
class MedicalConfig:
    def __init__(self):
        # åˆ›å»ºå¿…è¦çš„ç›®å½•
        self.model_dir = "trained_models"
        self.viz_dir = "charts"  # æ”¹ä¸ºchartsç›®å½•
        self.kg_dir = "knowledge_graphs"
        self.data_dir = "data"
        self.neo4j_dir = "neo4j_data"
        self.knowledge_base_dir = "knowledge_base"

        for dir_path in [self.model_dir, self.viz_dir, self.kg_dir, self.data_dir,
                         self.neo4j_dir, self.knowledge_base_dir]:
            os.makedirs(dir_path, exist_ok=True)

        logger.info(f"æ¨¡å‹ç›®å½•: {self.model_dir}")
        logger.info(f"å›¾è¡¨ç›®å½•: {self.viz_dir}")

        # è®­ç»ƒé…ç½®
        self.epochs = 10
        self.batch_size = 8
        self.learning_rate = 2e-5
        self.ddd_threshold = 1.0
        self.warmup_steps = 100
        self.max_grad_norm = 1.0
        self.diagnosis_threshold = 0.95

        # çŸ¥è¯†å›¾è°±é…ç½®
        self.kg_relation_types = [
            "has_symptom", "treated_with", "diagnosed_by",
            "symptom_of", "causes", "prevents", "contraindicates"
        ]

        # æ¨¡å‹é…ç½®
        self.pretrained_model_name = "bert-base-uncased"
        self.hidden_dropout_prob = 0.3
        self.max_seq_length = 128
        self.num_labels = 10

        # æ€è€ƒæ·±åº¦é…ç½®
        self.thinking_depth = 100
        self.certainty_threshold = 0.8

        # OLLAMAé…ç½®
        self.ollama_model = self.detect_ollama_models()
        self.ollama_base_url = "http://localhost:11434"

        # FAISSé…ç½®
        self.faiss_index_path = os.path.join(self.model_dir, "faiss_index.bin")
        self.embedding_model = "all-MiniLM-L6-v2"

        # çŸ¥è¯†å›¾è°±æ„å»ºæ–¹å¼
        self.kg_build_method = "auto"  # "auto" æˆ– "import"

        logger.info("\n=== åŒ»ç–—åˆ†æé…ç½® ===")
        logger.info(f"å›¾è¡¨ç›®å½•: {self.viz_dir}")
        logger.info(f"è¯Šæ–­é˜ˆå€¼: {self.diagnosis_threshold * 100}%")
        logger.info(f"è®­ç»ƒè½®æ•°: {self.epochs}")
        logger.info(f"æ€è€ƒæ·±åº¦: {self.thinking_depth}")
        logger.info(f"OLLAMAæ¨¡å‹: {self.ollama_model}")
        logger.info(f"çŸ¥è¯†å›¾è°±æ„å»ºæ–¹å¼: {self.kg_build_method}")
        logger.info("===================================")

    def detect_ollama_models(self):
        """æ£€æµ‹æœ¬åœ°å¯ç”¨çš„OLLAMAæ¨¡å‹"""
        try:
            # è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
            models = ollama.list()
            if models and 'models' in models:
                available_models = [model['name'] for model in models['models']]
                logger.info(f"æ£€æµ‹åˆ°å¯ç”¨OLLAMAæ¨¡å‹: {available_models}")

                # ä¼˜å…ˆé€‰æ‹©åŒ»ç–—ç›¸å…³æ¨¡å‹
                medical_models = [model for model in available_models
                                  if any(keyword in model.lower() for keyword in
                                         ['med', 'health', 'bio', 'science'])]

                if medical_models:
                    return medical_models[0]

                # å¦‚æœæ²¡æœ‰åŒ»ç–—ç›¸å…³æ¨¡å‹ï¼Œé€‰æ‹©è¾ƒå¤§çš„é€šç”¨æ¨¡å‹
                if available_models:
                    # ä¼˜å…ˆé€‰æ‹©è¾ƒå¤§çš„æ¨¡å‹
                    model_sizes = {
                        'llama2': 1, 'codellama': 2, 'mistral': 3,
                        'mixtral': 4, 'phi': 5, 'gemma': 6
                    }

                    sorted_models = sorted(available_models,
                                           key=lambda x: model_sizes.get(x.split(':')[0], 0),
                                           reverse=True)
                    return sorted_models[0]

            # é»˜è®¤æ¨¡å‹
            return "llama2"
        except Exception as e:
            logger.error(f"æ£€æµ‹OLLAMAæ¨¡å‹å¤±è´¥: {str(e)}")
            return "llama2"

    async def translate_to_english(self, text):
        """å¼‚æ­¥ç¿»è¯‘æ–‡æœ¬åˆ°è‹±æ–‡"""
        try:
            return await asyncio.to_thread(GoogleTranslator(source='auto', target='en').translate, text)
        except Exception as e:
            logger.error(f"ç¿»è¯‘é”™è¯¯: {str(e)}")
            return text

    async def translate_to_chinese(self, text):
        """å¼‚æ­¥ç¿»è¯‘æ–‡æœ¬åˆ°ä¸­æ–‡"""
        try:
            return await asyncio.to_thread(GoogleTranslator(source='auto', target='zh-CN').translate, text)
        except Exception as e:
            logger.error(f"ç¿»è¯‘é”™è¯¯: {str(e)}")
            return text

    def is_english(self, text):
        """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦ä¸ºè‹±æ–‡"""
        try:
            text.encode('ascii')
            return True
        except:
            return False

    def is_chinese(self, text):
        """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦ä¸ºä¸­æ–‡"""
        return any('\u4e00' <= char <= '\u9fff' for char in text)

    async def translate_bilingual(self, en_text, cn_text):
        """åˆ›å»ºåŒè¯­æ–‡æœ¬"""
        return f"ğŸŒ ENGLISH:\n{en_text}\n\nğŸŒ ä¸­æ–‡:\n{cn_text}"


# ========== æ–‡ä»¶å¤„ç†å·¥å…· ==========
class FileProcessor:
    """å¤„ç†å„ç§æ–‡ä»¶æ ¼å¼çš„å·¥å…·ç±»"""

    @staticmethod
    def extract_text_from_file(file_path):
        """ä»å„ç§æ–‡ä»¶æ ¼å¼ä¸­æå–æ–‡æœ¬"""
        try:
            text = ""
            ext = os.path.splitext(file_path)[1].lower()

            if ext == '.txt':
                with open(file_path, 'r', encoding='utf-8') as f:
                    text = f.read()

            elif ext == '.csv':
                df = pd.read_csv(file_path)
                text = df.to_string()

            elif ext == '.json':
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    text = json.dumps(data, ensure_ascii=False)

            elif ext == '.docx':
                doc = docx.Document(file_path)
                text = "\n".join([para.text for para in doc.paragraphs])

            elif ext == '.pdf':
                with open(file_path, 'rb') as f:
                    pdf_reader = PdfReader(f)
                    for page in pdf_reader.pages:
                        text += page.extract_text() + "\n"

            elif ext in ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']:
                # OCRå¤„ç†å›¾ç‰‡
                text = pytesseract.image_to_string(Image.open(file_path))

            elif ext in ['.html', '.htm']:
                with open(file_path, 'r', encoding='utf-8') as f:
                    soup = BeautifulSoup(f.read(), 'html.parser')
                    text = soup.get_text()

            elif ext in ['.xml']:
                tree = ET.parse(file_path)
                root = tree.getroot()
                text = ET.tostring(root, encoding='unicode', method='text')

            else:
                # å°è¯•ä½œä¸ºæ–‡æœ¬æ–‡ä»¶è¯»å–
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        text = f.read()
                except:
                    logger.warning(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path}")
                    return ""

            return text

        except Exception as e:
            logger.error(f"æ–‡ä»¶å¤„ç†é”™è¯¯ {file_path}: {str(e)}")
            return ""


# ========== FAISSå‘é‡æ•°æ®åº“ ==========
class FAISSVectorDB:
    def __init__(self, config):
        self.config = config
        self.index = None
        self.embedding_model = SentenceTransformer(config.embedding_model)
        self.documents = []

    def build_index(self, documents):
        """æ„å»ºFAISSç´¢å¼•"""
        self.documents = documents

        # ç”ŸæˆåµŒå…¥å‘é‡
        embeddings = self.embedding_model.encode(documents, show_progress_bar=True)

        # åˆ›å»ºFAISSç´¢å¼•
        dimension = embeddings.shape[1]
        self.index = faiss.IndexFlatL2(dimension)
        self.index.add(embeddings)

        # ä¿å­˜ç´¢å¼•
        faiss.write_index(self.index, self.config.faiss_index_path)
        logger.info(f"FAISSç´¢å¼•å·²æ„å»ºå¹¶ä¿å­˜: {self.config.faiss_index_path}")

    def load_index(self):
        """åŠ è½½FAISSç´¢å¼•"""
        if os.path.exists(self.config.faiss_index_path):
            self.index = faiss.read_index(self.config.faiss_index_path)
            logger.info(f"FAISSç´¢å¼•å·²åŠ è½½: {self.config.faiss_index_path}")
            return True
        return False

    def search(self, query, k=5):
        """æœç´¢ç›¸ä¼¼æ–‡æ¡£"""
        if self.index is None:
            if not self.load_index():
                return []

        # ç”ŸæˆæŸ¥è¯¢åµŒå…¥
        query_embedding = self.embedding_model.encode([query])

        # æœç´¢ç›¸ä¼¼æ–‡æ¡£
        distances, indices = self.index.search(query_embedding, k)

        # è¿”å›ç»“æœ
        results = []
        for i, idx in enumerate(indices[0]):
            if idx < len(self.documents):
                results.append({
                    'document': self.documents[idx],
                    'distance': distances[0][i]
                })

        return results


# ========== æœ¬åœ°çŸ¥è¯†å›¾è°±ç³»ç»Ÿ ==========
class LocalKnowledgeGraph:
    """æœ¬åœ°çŸ¥è¯†å›¾è°±å®ç°"""

    def __init__(self, config):
        self.config = config
        self.graph = nx.MultiDiGraph()
        self.entity_types = ["disease", "symptom", "medication", "test", "anatomy"]
        self.relation_counter = {rel: 0 for rel in self.config.kg_relation_types}
        self.entity_dict = {}
        self.entity_name_to_id = {}
        self.graph_file = os.path.join(config.neo4j_dir, "local_knowledge_graph.pkl")

    def add_entity(self, entity_id, entity_type, properties=None):
        """æ·»åŠ å®ä½“åˆ°çŸ¥è¯†å›¾è°±"""
        if entity_type not in self.entity_types:
            logger.warning(f"æœªçŸ¥å®ä½“ç±»å‹: {entity_type}")
            return False

        if properties is None:
            properties = {}

        properties['type'] = entity_type
        self.graph.add_node(entity_id, **properties)

        # æ›´æ–°æ˜ å°„
        self.entity_dict[entity_id] = properties
        if 'name' in properties:
            self.entity_name_to_id[properties['name']] = entity_id

        return True

    def add_relation(self, source_id, target_id, relation_type, properties=None):
        """æ·»åŠ å…³ç³»åˆ°çŸ¥è¯†å›¾è°±"""
        if relation_type not in self.config.kg_relation_types:
            logger.warning(f"æœªçŸ¥å…³ç³»ç±»å‹: {relation_type}")
            return False

        if properties is None:
            properties = {}

        self.graph.add_edge(source_id, target_id, key=relation_type, **properties)
        self.relation_counter[relation_type] += 1
        return True

    def find_entities(self, entity_name, entity_type=None):
        """æ ¹æ®åç§°æŸ¥æ‰¾å®ä½“"""
        results = []
        for node, data in self.graph.nodes(data=True):
            if 'name' in data and (entity_name.lower() in data['name'].lower() or
                                   difflib.SequenceMatcher(None, entity_name.lower(),
                                                           data['name'].lower()).ratio() > 0.7):
                if entity_type is None or data.get('type') == entity_type:
                    results.append((node, data))
        return results

    def find_related_entities(self, entity_id, relation_type=None, max_depth=1):
        """æŸ¥æ‰¾ç›¸å…³å®ä½“"""
        related_entities = set()
        queue = deque([(entity_id, 0)])
        visited = set([entity_id])

        while queue:
            current_id, depth = queue.popleft()

            if depth > max_depth:
                continue

            # è·å–æ‰€æœ‰å‡ºè¾¹å’Œå…¥è¾¹
            for _, neighbor, key, data in self.graph.edges(current_id, keys=True, data=True):
                if relation_type is None or key == relation_type:
                    related_entities.add(neighbor)

                if neighbor not in visited and depth < max_depth:
                    visited.add(neighbor)
                    queue.append((neighbor, depth + 1))

            for neighbor, _, key, data in self.graph.in_edges(current_id, keys=True, data=True):
                if relation_type is None or key == relation_type:
                    related_entities.add(neighbor)

                if neighbor not in visited and depth < max_depth:
                    visited.add(neighbor)
                    queue.append((neighbor, depth + 1))

        return [(entity, self.graph.nodes[entity]) for entity in related_entities]

    def visualize(self, filename="knowledge_graph.png"):
        """å¯è§†åŒ–çŸ¥è¯†å›¾è°±"""
        if len(self.graph.nodes()) == 0:
            logger.warning("çŸ¥è¯†å›¾è°±ä¸ºç©ºï¼Œæ— æ³•å¯è§†åŒ–")
            return None

        plt.figure(figsize=(20, 15))

        # æ ¹æ®å®ä½“ç±»å‹è®¾ç½®é¢œè‰²
        node_colors = []
        for node in self.graph.nodes():
            node_type = self.graph.nodes[node].get('type', 'unknown')
            if node_type == "disease":
                node_colors.append('red')
            elif node_type == "symptom":
                node_colors.append('blue')
            elif node_type == "medication":
                node_colors.append('green')
            elif node_type == "test":
                node_colors.append('orange')
            elif node_type == "anatomy":
                node_colors.append('purple')
            else:
                node_colors.append('gray')

        # ç»˜åˆ¶çŸ¥è¯†å›¾è°±
        pos = nx.spring_layout(self.graph, k=1, iterations=50)
        nx.draw_networkx_nodes(self.graph, pos, node_color=node_colors, node_size=500, alpha=0.8)

        # ç»˜åˆ¶è¾¹
        edge_colors = []
        for u, v, key in self.graph.edges(keys=True):
            if key == "has_symptom":
                edge_colors.append('blue')
            elif key == "treated_with":
                edge_colors.append('green')
            elif key == "diagnosed_by":
                edge_colors.append('orange')
            elif key == "symptom_of":
                edge_colors.append('lightblue')
            elif key == "causes":
                edge_colors.append('red')
            elif key == "prevents":
                edge_colors.append('lightgreen')
            elif key == "contraindicates":
                edge_colors.append('pink')
            else:
                edge_colors.append('gray')

        nx.draw_networkx_edges(self.graph, pos, edge_color=edge_colors, alpha=0.5)

        # æ·»åŠ æ ‡ç­¾
        labels = {}
        for node in self.graph.nodes():
            labels[node] = self.graph.nodes[node].get('name', node)[:15]

        nx.draw_networkx_labels(self.graph, pos, labels, font_size=8)

        # æ·»åŠ å…³ç³»ç±»å‹æ ‡ç­¾
        edge_labels = {}
        for u, v, key in self.graph.edges(keys=True):
            edge_labels[(u, v)] = key

        nx.draw_networkx_edge_labels(self.graph, pos, edge_labels, font_size=6)

        plt.title("Medical Knowledge Graph")
        plt.axis('off')
        plt.tight_layout()

        # ä¿å­˜å›¾åƒ
        filepath = os.path.join(self.config.viz_dir, filename)
        plt.savefig(filepath, dpi=300, bbox_inches='tight')
        plt.close()

        logger.info(f"çŸ¥è¯†å›¾è°±å·²ä¿å­˜: {filepath}")
        return filepath

    def export_to_json(self, filename="knowledge_graph.json"):
        """å¯¼å‡ºçŸ¥è¯†å›¾è°±åˆ°JSONæ–‡ä»¶"""
        data = {
            "nodes": [],
            "edges": []
        }

        # æ·»åŠ èŠ‚ç‚¹
        for node, node_data in self.graph.nodes(data=True):
            node_data["id"] = node
            data["nodes"].append(node_data)

        # æ·»åŠ è¾¹
        for u, v, key, edge_data in self.graph.edges(data=True, keys=True):
            edge_data.update({
                "source": u,
                "target": v,
                "type": key
            })
            data["edges"].append(edge_data)

        # ä¿å­˜åˆ°æ–‡ä»¶
        filepath = os.path.join(self.config.kg_dir, filename)
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        logger.info(f"çŸ¥è¯†å›¾è°±å·²å¯¼å‡º: {filepath}")
        return filepath

    def save(self):
        """ä¿å­˜çŸ¥è¯†å›¾è°±åˆ°æ–‡ä»¶"""
        graph_data = {
            'graph': self.graph,
            'entity_dict': self.entity_dict,
            'entity_name_to_id': self.entity_name_to_id,
            'relation_counter': self.relation_counter
        }

        with open(self.graph_file, 'wb') as f:
            pickle.dump(graph_data, f)

        logger.info(f"çŸ¥è¯†å›¾è°±å·²ä¿å­˜åˆ°: {self.graph_file}")

    def load(self):
        """ä»æ–‡ä»¶åŠ è½½çŸ¥è¯†å›¾è°±"""
        if os.path.exists(self.graph_file):
            with open(self.graph_file, 'rb') as f:
                graph_data = pickle.load(f)

            self.graph = graph_data['graph']
            self.entity_dict = graph_data['entity_dict']
            self.entity_name_to_id = graph_data['entity_name_to_id']
            self.relation_counter = graph_data['relation_counter']

            logger.info(f"çŸ¥è¯†å›¾è°±å·²ä» {self.graph_file} åŠ è½½")
            return True
        return False

    def import_from_file(self, file_path):
        """ä»æ–‡ä»¶å¯¼å…¥çŸ¥è¯†å›¾è°±"""
        try:
            file_processor = FileProcessor()
            content = file_processor.extract_text_from_file(file_path)

            if not content:
                logger.error(f"æ— æ³•ä»æ–‡ä»¶æå–å†…å®¹: {file_path}")
                return False

            # ä½¿ç”¨OLLAMAåˆ†æå†…å®¹å¹¶æ„å»ºçŸ¥è¯†å›¾è°±
            return self.build_with_ollama(content, file_path)

        except Exception as e:
            logger.error(f"å¯¼å…¥çŸ¥è¯†å›¾è°±å¤±è´¥: {str(e)}")
            return False

    def build_with_ollama(self, content, source_info):
        """ä½¿ç”¨OLLAMAåˆ†æå†…å®¹å¹¶æ„å»ºçŸ¥è¯†å›¾è°±"""
        try:
            # ä½¿ç”¨OLLAMAæå–åŒ»ç–—å®ä½“å’Œå…³ç³»
            prompt = f"""
            è¯·ä»ä»¥ä¸‹åŒ»ç–—æ–‡æœ¬ä¸­æå–ç–¾ç—…ã€ç—‡çŠ¶ã€è¯ç‰©ã€æ£€æŸ¥å’Œèº«ä½“éƒ¨ä½ç­‰å®ä½“ï¼Œä»¥åŠå®ƒä»¬ä¹‹é—´çš„å…³ç³»ã€‚
            æ–‡æœ¬å†…å®¹:
            {content[:4000]}  # é™åˆ¶æ–‡æœ¬é•¿åº¦

            è¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æœï¼ŒåŒ…å«ä»¥ä¸‹ç»“æ„:
            {{
                "entities": [
                    {{
                        "type": "ç–¾ç—…/ç—‡çŠ¶/è¯ç‰©/æ£€æŸ¥/èº«ä½“éƒ¨ä½",
                        "name": "å®ä½“åç§°",
                        "properties": {{}}
                    }}
                ],
                "relations": [
                    {{
                        "source": "æºå®ä½“åç§°",
                        "target": "ç›®æ ‡å®ä½“åç§°",
                        "type": "å…³ç³»ç±»å‹",
                        "properties": {{}}
                    }}
                ]
            }}
            """

            response = ollama.chat(
                model=self.config.ollama_model,
                messages=[{'role': 'user', 'content': prompt}]
            )

            result_text = response['message']['content']

            # æå–JSONéƒ¨åˆ†
            json_match = re.search(r'\{[\s\S]*\}', result_text)
            if json_match:
                result_data = json.loads(json_match.group())

                # åˆ›å»ºå®ä½“æ˜ å°„
                entity_map = {}
                for entity in result_data.get('entities', []):
                    entity_id = f"{entity['type']}_{len(entity_map)}"
                    entity_map[entity['name']] = entity_id
                    self.add_entity(entity_id, entity['type'], {
                        'name': entity['name'],
                        'source': source_info,
                        **entity.get('properties', {})
                    })

                # åˆ›å»ºå…³ç³»
                for relation in result_data.get('relations', []):
                    source_id = entity_map.get(relation['source'])
                    target_id = entity_map.get(relation['target'])

                    if source_id and target_id:
                        self.add_relation(
                            source_id,
                            target_id,
                            relation['type'],
                            relation.get('properties', {})
                        )

                logger.info(
                    f"ä½¿ç”¨OLLAMAä» {source_info} æå–äº† {len(entity_map)} ä¸ªå®ä½“å’Œ {len(result_data.get('relations', []))} ä¸ªå…³ç³»")
                return True
            else:
                logger.error("OLLAMAå“åº”ä¸­æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„JSONæ•°æ®")
                return False

        except Exception as e:
            logger.error(f"ä½¿ç”¨OLLAMAæ„å»ºçŸ¥è¯†å›¾è°±å¤±è´¥: {str(e)}")
            return False

    def validate_graph(self):
        """éªŒè¯çŸ¥è¯†å›¾è°±çš„å®Œæ•´æ€§"""
        issues = []

        # æ£€æŸ¥å­¤ç«‹èŠ‚ç‚¹
        isolated_nodes = list(nx.isolates(self.graph))
        if isolated_nodes:
            issues.append(f"å‘ç° {len(isolated_nodes)} ä¸ªå­¤ç«‹èŠ‚ç‚¹")

        # æ£€æŸ¥é‡å¤å®ä½“
        name_count = {}
        for node, data in self.graph.nodes(data=True):
            if 'name' in data:
                name = data['name']
                name_count[name] = name_count.get(name, 0) + 1

        duplicates = {name: count for name, count in name_count.items() if count > 1}
        if duplicates:
            issues.append(f"å‘ç° {len(duplicates)} ä¸ªé‡å¤å®ä½“åç§°")

        # æ£€æŸ¥æ— æ•ˆå…³ç³»
        for u, v, key in self.graph.edges(keys=True):
            if key not in self.config.kg_relation_types:
                issues.append(f"å‘ç°æ— æ•ˆå…³ç³»ç±»å‹: {key}")

        return issues if issues else ["çŸ¥è¯†å›¾è°±éªŒè¯é€šè¿‡ï¼Œæ— å‘ç°é—®é¢˜"]


# ========== çŸ¥è¯†å›¾è°±å·¥å‚ ==========
class KnowledgeGraphFactory:
    """çŸ¥è¯†å›¾è°±å·¥å‚ï¼Œæ ¹æ®é…ç½®è¿”å›é€‚å½“çš„çŸ¥è¯†å›¾è°±å®ä¾‹"""

    @staticmethod
    def create_knowledge_graph(config):
        return LocalKnowledgeGraph(config)


# ========== åŒ»ç–—æ•°æ®é›† ==========
class MedicalDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]

        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_len,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt',
        )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }


# ========== çŸ¥è¯†åº“ ==========
class MedicalKnowledgeBase:
    def __init__(self, config):
        self.config = config
        self.disease_info = {}
        self.symptom_info = {}
        self.medication_ddd_info = {}
        self.full_knowledge = []
        self.knowledge_graph = KnowledgeGraphFactory.create_knowledge_graph(config)
        self.faiss_db = FAISSVectorDB(config)
        self.learning_stats = {
            "files_processed": 0,
            "diseases_extracted": 0,
            "symptoms_extracted": 0,
            "medications_extracted": 0,
            "tests_extracted": 0,
            "total_size_kb": 0,
            "kg_entities": 0,
            "kg_relations": 0
        }

        # åŠ è½½çŸ¥è¯†
        self.load_knowledge()
        logger.info(f"çŸ¥è¯†åº“åŠ è½½å®Œæˆ: {len(self.disease_info)}ç§ç–¾ç—…, "
                    f"{len(self.symptom_info)}ç§ç—‡çŠ¶, "
                    f"{self.learning_stats['files_processed']}ä¸ªæ–‡ä»¶, "
                    f"{self.learning_stats['total_size_kb']:.2f} KBå†…å®¹, "
                    f"{self.learning_stats['kg_entities']}ä¸ªçŸ¥è¯†å›¾è°±å®ä½“, "
                    f"{self.learning_stats['kg_relations']}ä¸ªçŸ¥è¯†å›¾è°±å…³ç³»")

    def load_knowledge(self):
        """ä»çŸ¥è¯†åº“æ–‡ä»¶å¤¹åŠ è½½æ‰€æœ‰çŸ¥è¯†åº“æ–‡ä»¶"""
        logger.info("ä»æœ¬åœ°çŸ¥è¯†åº“æ–‡ä»¶å¤¹åŠ è½½åŒ»ç–—çŸ¥è¯†...")

        # è¯¢é—®ç”¨æˆ·çŸ¥è¯†å›¾è°±æ„å»ºæ–¹å¼
        print("\nè¯·é€‰æ‹©çŸ¥è¯†å›¾è°±æ„å»ºæ–¹å¼:")
        print("1. è‡ªåŠ¨ä»çŸ¥è¯†åº“å­¦ä¹ ç”ŸæˆçŸ¥è¯†å›¾è°±")
        print("2. å¯¼å…¥å·²æœ‰çš„çŸ¥è¯†å›¾è°±æ–‡ä»¶")
        choice = input("è¯·è¾“å…¥é€‰æ‹© (1 æˆ– 2ï¼Œç›´æ¥å›è½¦é»˜è®¤é€‰æ‹©1): ").strip()

        if choice == "2":
            import_file = input("è¯·è¾“å…¥çŸ¥è¯†å›¾è°±æ–‡ä»¶è·¯å¾„: ").strip()
            if os.path.exists(import_file):
                if self.knowledge_graph.import_from_file(import_file):
                    logger.info("çŸ¥è¯†å›¾è°±å¯¼å…¥æˆåŠŸ")
                    # éªŒè¯çŸ¥è¯†å›¾è°±
                    validation_issues = self.knowledge_graph.validate_graph()
                    for issue in validation_issues:
                        logger.info(f"çŸ¥è¯†å›¾è°±éªŒè¯: {issue}")

                    # å¯è§†åŒ–çŸ¥è¯†å›¾è°±
                    self.knowledge_graph.visualize()
                    self.knowledge_graph.export_to_json()
                    self.knowledge_graph.save()
                    return
                else:
                    logger.error("çŸ¥è¯†å›¾è°±å¯¼å…¥å¤±è´¥ï¼Œå°†ä½¿ç”¨è‡ªåŠ¨å­¦ä¹ æ¨¡å¼")
            else:
                logger.error("æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†ä½¿ç”¨è‡ªåŠ¨å­¦ä¹ æ¨¡å¼")

        # è‡ªåŠ¨å­¦ä¹ æ¨¡å¼
        logger.info("ä½¿ç”¨è‡ªåŠ¨å­¦ä¹ æ¨¡å¼æ„å»ºçŸ¥è¯†å›¾è°±...")

        # å°è¯•åŠ è½½å·²æœ‰çš„çŸ¥è¯†å›¾è°±
        if self.knowledge_graph.load():
            logger.info("æˆåŠŸåŠ è½½å·²æœ‰çš„çŸ¥è¯†å›¾è°±")
            return

        file_processor = FileProcessor()
        documents = []  # ç”¨äºFAISSç´¢å¼•çš„æ–‡æ¡£

        # å¤„ç†çŸ¥è¯†åº“ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶
        for root, _, files in os.walk(self.config.knowledge_base_dir):
            for file in files:
                file_path = os.path.join(root, file)
                logger.info(f"å¤„ç†æ–‡ä»¶: {file_path}")

                try:
                    content = file_processor.extract_text_from_file(file_path)
                    if content:
                        # ä½¿ç”¨OLLAMAåˆ†æå†…å®¹å¹¶æ„å»ºçŸ¥è¯†å›¾è°±
                        self.knowledge_graph.build_with_ollama(content, file_path)
                        documents.append(content)
                        self.learning_stats["files_processed"] += 1
                        self.learning_stats["total_size_kb"] += len(content.encode('utf-8')) / 1024
                except Exception as e:
                    logger.error(f"æ–‡ä»¶å¤„ç†é”™è¯¯ {file_path}: {str(e)}")

        # æ„å»ºFAISSç´¢å¼•
        if documents:
            self.faiss_db.build_index(documents)

        # å¯è§†åŒ–çŸ¥è¯†å›¾è°±
        if self.knowledge_graph.graph.number_of_nodes() > 0:
            self.knowledge_graph.visualize()
            self.knowledge_graph.export_to_json()
            self.knowledge_graph.save()

            # æ›´æ–°å­¦ä¹ ç»Ÿè®¡
            self.learning_stats["kg_entities"] = self.knowledge_graph.graph.number_of_nodes()
            self.learning_stats["kg_relations"] = self.knowledge_graph.graph.number_of_edges()

        if self.learning_stats["files_processed"] == 0:
            logger.warning("çŸ¥è¯†åº“æœªåŠ è½½ä»»ä½•å†…å®¹")

    def prepare_training_data(self):
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        texts = []
        labels = []
        label_map = {}

        # ä»çŸ¥è¯†å›¾è°±ä¸­æå–è®­ç»ƒæ•°æ®
        for node, data in self.knowledge_graph.graph.nodes(data=True):
            if data.get('type') == 'disease' and 'name' in data:
                disease_name = data['name']

                if disease_name not in label_map:
                    label_map[disease_name] = len(label_map)

                # è·å–ç›¸å…³ç—‡çŠ¶
                symptoms = []
                for _, neighbor, key in self.knowledge_graph.graph.edges(node, keys=True):
                    if key == 'has_symptom':
                        neighbor_data = self.knowledge_graph.graph.nodes[neighbor]
                        if 'name' in neighbor_data:
                            symptoms.append(neighbor_data['name'])

                if symptoms:
                    symptoms_text = ", ".join(symptoms)
                    texts.append(symptoms_text)
                    labels.append(label_map[disease_name])

                    disease_text = f"{disease_name} with symptoms: {symptoms_text}"
                    texts.append(disease_text)
                    labels.append(label_map[disease_name])

        return texts, labels, label_map

    def rag_search(self, query, k=5):
        """ä½¿ç”¨RAGæ£€ç´¢ç›¸å…³çŸ¥è¯†"""
        return self.faiss_db.search(query, k)

    def kg_query(self, query):
        """æ‰§è¡ŒçŸ¥è¯†å›¾è°±æŸ¥è¯¢"""
        # ç®€å•å…³é”®è¯æŸ¥è¯¢
        results = []
        for entity_name in self.knowledge_graph.entity_name_to_id.keys():
            if query.lower() in entity_name.lower():
                entity_id = self.knowledge_graph.entity_name_to_id[entity_name]
                entity_data = self.knowledge_graph.entity_dict[entity_id]
                results.append({
                    'entity': entity_data,
                    'related': self.knowledge_graph.find_related_entities(entity_id)
                })
        return results

    def validate_knowledge_graph(self):
        """éªŒè¯çŸ¥è¯†å›¾è°±çš„å®Œæ•´æ€§"""
        return self.knowledge_graph.validate_graph()


# ========== åŒ»ç–—AIæ¨¡å‹ ==========
class MedicalAIModel(nn.Module):
    def __init__(self, config, num_labels):
        super(MedicalAIModel, self).__init__()
        self.config = config
        self.bert = BertModel.from_pretrained(config.pretrained_model_name)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)
        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)
        self.attention_weights = None

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs.pooler_output
        pooled_output = self.dropout(pooled_output)

        self.attention_weights = outputs.last_hidden_state

        logits = self.classifier(pooled_output)
        return logits


# ========== è®­ç»ƒç›‘æ§å™¨ ==========
class TrainingMonitor:
    def __init__(self, config):
        self.config = config
        self.train_losses = []
        self.val_losses = []
        self.train_accuracies = []
        self.val_accuracies = []
        self.learning_rates = []
        self.cpu_usages = []
        self.gpu_usages = []
        self.memory_usages = []
        self.timestamps = []

    def update(self, train_loss, val_loss, train_acc, val_acc, lr):
        """æ›´æ–°è®­ç»ƒæŒ‡æ ‡"""
        self.train_losses.append(train_loss)
        self.val_losses.append(val_loss)
        self.train_accuracies.append(train_acc)
        self.val_accuracies.append(val_acc)
        self.learning_rates.append(lr)
        self.cpu_usages.append(psutil.cpu_percent())

        gpus = GPUtil.getGPUs()
        if gpus:
            self.gpu_usages.append(gpus[0].load * 100)
        else:
            self.gpu_usages.append(0)

        self.memory_usages.append(psutil.virtual_memory().percent)
        self.timestamps.append(datetime.now())

    def plot_learning_curves(self, filename="learning_curves.png"):
        """ç»˜åˆ¶å­¦ä¹ æ›²çº¿"""
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))

        ax1.plot(self.train_losses, label='Training Loss')
        ax1.plot(self.val_losses, label='Validation Loss')
        ax1.set_title('Training and Validation Loss')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.legend()
        ax1.grid(True)

        ax2.plot(self.train_accuracies, label='Training Accuracy')
        ax2.plot(self.val_accuracies, label='Validation Accuracy')
        ax2.set_title('Training and Validation Accuracy')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Accuracy')
        ax2.legend()
        ax2.grid(True)

        ax3.plot(self.learning_rates, label='Learning Rate', color='green')
        ax3.set_title('Learning Rate Schedule')
        ax3.set_xlabel('Epoch')
        ax3.set_ylabel('Learning Rate')
        ax3.legend()
        ax3.grid(True)

        ax4.plot(self.cpu_usages, label='CPU Usage', color='red')
        ax4.plot(self.gpu_usages, label='GPU Usage', color='blue')
        ax4.plot(self.memory_usages, label='Memory Usage', color='green')
        ax4.set_title('Resource Usage')
        ax4.set_xlabel('Epoch')
        ax4.set_ylabel('Usage (%)')
        ax4.legend()
        ax4.grid(True)

        plt.tight_layout()

        filepath = os.path.join(self.config.viz_dir, filename)
        plt.savefig(filepath, dpi=300, bbox_inches='tight')
        plt.close()

        logger.info(f"å­¦ä¹ æ›²çº¿å·²ä¿å­˜: {filepath}")
        return filepath

    def plot_attention_heatmap(self, attention_weights, tokens, filename="attention_heatmap.png"):
        """ç»˜åˆ¶æ³¨æ„åŠ›çƒ­åŠ›å›¾"""
        plt.figure(figsize=(12, 8))

        avg_attention = attention_weights.mean(dim=1).squee().cpu().detach().numpy()

        sns.heatmap(avg_attention, xticklabels=tokens[:avg_attention.shape[1]],
                    yticklabels=[f"Head {i + 1}" for i in range(avg_attention.shape[0])],
                    cmap="YlOrRd")
        plt.title('Attention Heatmap')
        plt.xticks(rotation=45, ha='right')
        plt.tight_layout()

        filepath = os.path.join(self.config.viz_dir, filename)
        plt.savefig(filepath, dpi=300, bbox_inches='tight')
        plt.close()

        logger.info(f"æ³¨æ„åŠ›çƒ­åŠ›å›¾å·²ä¿å­˜: {filepath}")
        return filepath

    def plot_thinking_process(self, thinking_steps, certainty_scores, filename="thinking_process.png"):
        """ç»˜åˆ¶æ€è€ƒè¿‡ç¨‹å›¾"""
        plt.figure(figsize=(12, 6))

        steps = [f"Step {i + 1}" for i in range(len(thinking_steps))]
        positions = np.arange(len(steps))

        plt.bar(positions, certainty_scores, color='skyblue', alpha=0.7)
        plt.plot(positions, certainty_scores, marker='o', color='red', linewidth=2)

        plt.xlabel('Thinking Steps')
        plt.ylabel('Certainty Score')
        plt.title('AI Thinking Process and Certainty Progression')
        plt.xticks(positions, steps)
        plt.ylim(0, 1)
        plt.grid(True, axis='y', alpha=0.3)

        for i, v in enumerate(certainty_scores):
            plt.text(i, v + 0.02, f"{v:.2f}", ha='center', va='bottom')

        plt.tight_layout()

        filepath = os.path.join(self.config.viz_dir, filename)
        plt.savefig(filepath, dpi=300, bbox_inches='tight')
        plt.close()

        logger.info(f"æ€è€ƒè¿‡ç¨‹å›¾å·²ä¿å­˜: {filepath}")
        return filepath

    def plot_resource_usage(self, filename="resource_usage.png"):
        """ç»˜åˆ¶èµ„æºä½¿ç”¨æƒ…å†µå›¾"""
        plt.figure(figsize=(12, 6))

        time_points = range(len(self.cpu_usages))

        plt.plot(time_points, self.cpu_usages, label='CPU Usage', color='red', linewidth=2)
        plt.plot(time_points, self.gpu_usages, label='GPU Usage', color='blue', linewidth=2)
        plt.plot(time_points, self.memory_usages, label='Memory Usage', color='green', linewidth=2)

        plt.xlabel('Time')
        plt.ylabel('Usage (%)')
        plt.title('Resource Usage During Training')
        plt.legend()
        plt.grid(True, alpha=0.3)

        plt.tight_layout()

        filepath = os.path.join(self.config.viz_dir, filename)
        plt.savefig(filepath, dpi=300, bbox_inches='tight')
        plt.close()

        logger.info(f"èµ„æºä½¿ç”¨å›¾å·²ä¿å­˜: {filepath}")
        return filepath


# ========== åŒ»ç–—åŠ©æ‰‹ ==========
class MedicalAssistant:
    def __init__(self, knowledge_base, config, model, tokenizer, label_map):
        self.knowledge_base = knowledge_base
        self.config = config
        self.model = model
        self.tokenizer = tokenizer
        self.label_map = label_map
        self.reverse_label_map = {v: k for k, v in label_map.items()}
        self.thought_process = []
        self.current_symptoms = []
        self.attempt_count = 0
        self.session_id = datetime.now().strftime("%Y%m%d%H%M%S")
        self.thinking_steps = []
        self.certainty_scores = []
        self.training_monitor = TrainingMonitor(config)

    async def ollama_query(self, prompt):
        """ä½¿ç”¨OLLAMAæœ¬åœ°æ¨¡å‹è¿›è¡ŒæŸ¥è¯¢"""
        try:
            response = ollama.chat(
                model=self.config.ollama_model,
                messages=[{'role': 'user', 'content': prompt}]
            )
            return response['message']['content']
        except Exception as e:
            logger.error(f"OLLAMAæŸ¥è¯¢é”™è¯¯: {str(e)}")
            return "æ— æ³•ä½¿ç”¨æœ¬åœ°æ¨¡å‹è¿›è¡Œæ¨ç†"

    async def deep_think(self, symptoms, depth=0, max_depth=100, current_certainty=0.0):
        """æ·±åº¦æ€è€ƒè¿‡ç¨‹ï¼Œä½¿ç”¨OLLAMAæ¨¡å‹è¿›è¡Œæ¨ç†"""
        if depth >= max_depth:
            return current_certainty, symptoms

        # è®°å½•æ€è€ƒæ­¥éª¤
        step_info = {
            "depth": depth,
            "symptoms": symptoms.copy(),
            "certainty": current_certainty,
            "timestamp": datetime.now()
        }
        self.thinking_steps.append(step_info)
        self.certainty_scores.append(current_certainty)

        # ä½¿ç”¨çŸ¥è¯†å›¾è°±æ‰©å±•ç—‡çŠ¶
        expanded_symptoms = await self.expand_symptoms_with_kg(symptoms)

        # ä½¿ç”¨OLLAMAæ¨¡å‹è¿›è¡Œæ¨ç†
        thinking_prompt = f"ä½œä¸ºåŒ»ç–—ä¸“å®¶ï¼Œåˆ†æä»¥ä¸‹ç—‡çŠ¶: {', '.join(expanded_symptoms)}ã€‚è¯·æ€è€ƒå¯èƒ½çš„ç–¾ç—…å¹¶ç»™å‡ºç½®ä¿¡åº¦ã€‚"
        ollama_response = await self.ollama_query(thinking_prompt)

        # è®°å½•AIæ€è€ƒè¿‡ç¨‹
        self.thought_process.append(f"æ€è€ƒæ­¥éª¤ {depth + 1}: {ollama_response}")

        # ä½¿ç”¨æ¨¡å‹è¿›è¡Œè¯Šæ–­
        diagnosis_result = await self.model_based_diagnosis(expanded_symptoms)

        # è®¡ç®—æ–°çš„ç¡®å®šæ€§
        new_certainty = diagnosis_result["confidence"]

        # å¦‚æœç¡®å®šæ€§è¶³å¤Ÿé«˜ï¼Œåœæ­¢é€’å½’
        if new_certainty >= self.config.certainty_threshold:
            return new_certainty, expanded_symptoms

        # å¦åˆ™ç»§ç»­æ·±å…¥æ€è€ƒ
        return await self.deep_think(expanded_symptoms, depth + 1, max_depth, new_certainty)

    async def expand_symptoms_with_kg(self, symptoms):
        """ä½¿ç”¨çŸ¥è¯†å›¾è°±æ‰©å±•ç—‡çŠ¶åˆ—è¡¨"""
        expanded_symptoms = symptoms.copy()

        for symptom in symptoms:
            # æŸ¥æ‰¾ç›¸å…³ç—‡çŠ¶
            related_entities = self.knowledge_graph.find_related_entities(symptom)
            for entity, data in related_entities:
                if data.get('type') == 'symptom' and 'name' in data and data['name'] not in expanded_symptoms:
                    expanded_symptoms.append(data['name'])

        return expanded_symptoms

    async def diagnose(self, chief_complaint):
        """è¯Šæ–­æµç¨‹"""
        self.thought_process = [f"æ‚£è€…ä¸»è¯‰: {chief_complaint}"]
        self.thinking_steps = []
        self.certainty_scores = []

        # æ­¥éª¤1: ä½¿ç”¨RAGæ£€ç´¢ç›¸å…³çŸ¥è¯†
        rag_results = self.knowledge_base.rag_search(chief_complaint, k=3)
        if rag_results:
            self.thought_process.append("RAGæ£€ç´¢åˆ°çš„ç›¸å…³çŸ¥è¯†:")
            for i, result in enumerate(rag_results):
                self.thought_process.append(f"ç›¸å…³æ–‡æ¡£ {i + 1}: {result['document'][:100]}...")

        # æ­¥éª¤2: ä½¿ç”¨çŸ¥è¯†å›¾è°±æŸ¥è¯¢ç›¸å…³ä¿¡æ¯
        kg_results = self.knowledge_base.kg_query(chief_complaint)
        if kg_results:
            self.thought_process.append("çŸ¥è¯†å›¾è°±æŸ¥è¯¢ç»“æœ:")
            for i, result in enumerate(kg_results[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ªç»“æœ
                self.thought_process.append(f"çŸ¥è¯†å›¾è°±ç»“æœ {i + 1}: {str(result)[:100]}...")

        # æ­¥éª¤3: æ·±åº¦æ€è€ƒè¿‡ç¨‹
        initial_symptoms = [chief_complaint]
        final_certainty, final_symptoms = await self.deep_think(
            initial_symptoms,
            max_depth=self.config.thinking_depth
        )

        self.thought_process.append(f"æ·±åº¦æ€è€ƒå®Œæˆ: {len(self.thinking_steps)} æ­¥éª¤")
        self.thought_process.append(f"æœ€ç»ˆç¡®å®šæ€§: {final_certainty:.2f}")

        # æ­¥éª¤4: ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œè¯Šæ–­
        diagnosis = await self.model_based_diagnosis(final_symptoms)

        disease_en = await self.config.translate_to_english(diagnosis['disease'])
        self.thought_process.append(
            f"æ¨¡å‹è¯Šæ–­: {diagnosis['disease']}/{disease_en} (ç½®ä¿¡åº¦: {diagnosis['confidence'] * 100:.1f}%)")

        # æ­¥éª¤5: ç”Ÿæˆæœ€ç»ˆå“åº”
        return await self.generate_final_response(diagnosis)

    async def model_based_diagnosis(self, symptoms):
        """ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œè¯Šæ–­"""
        symptoms_text = ", ".join(symptoms)

        self.model.eval()

        encoding = self.tokenizer.encode_plus(
            symptoms_text,
            add_special_tokens=True,
            max_length=self.config.max_seq_length,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt',
        )

        with torch.no_grad():
            input_ids = encoding['input_ids']
            attention_mask = encoding['attention_mask']

            outputs = self.model(input_ids, attention_mask)
            probabilities = torch.softmax(outputs, dim=1)
            confidence, predicted_idx = torch.max(probabilities, dim=1)

            disease = self.reverse_label_map[predicted_idx.item()]
            confidence_value = confidence.item()

        return {
            "disease": disease,
            "confidence": confidence_value
        }

    async def generate_final_response(self, diagnosis):
        """ç”Ÿæˆæœ€ç»ˆå“åº”"""
        # ä¸­æ–‡éƒ¨åˆ†
        cn_response = f"è¯Šæ–­ç»“æœ:\n"
        cn_response += f"ç–¾ç—…: {diagnosis['disease']}\n"
        cn_response += f"ç½®ä¿¡åº¦: {diagnosis['confidence'] * 100:.1f}%\n\n"

        # è‹±æ–‡éƒ¨åˆ†
        en_disease = await self.config.translate_to_english(diagnosis['disease'])

        en_response = f"Diagnosis:\n"
        en_response += f"Disease: {en_disease}\n"
        en_response += f"Confidence: {diagnosis['confidence'] * 100:.1f}%\n\n"

        # ç»„åˆä¸­è‹±æ–‡å“åº”
        final_response = f"ğŸŒ ä¸­æ–‡:\n{cn_response}\n\n"
        final_response += f"ğŸŒ ENGLISH:\n{en_response}"

        return final_response


# ========== è®­ç»ƒå‡½æ•° ==========
async def train_model(config, knowledge_base):
    """è®­ç»ƒåŒ»ç–—è¯Šæ–­æ¨¡å‹"""
    logger.info("å¼€å§‹è®­ç»ƒåŒ»ç–—è¯Šæ–­æ¨¡å‹...")

    texts, labels, label_map = knowledge_base.prepare_training_data()

    if len(texts) == 0:
        logger.error("æ²¡æœ‰è¶³å¤Ÿçš„è®­ç»ƒæ•°æ®")
        return None, None, None, None

    train_texts, val_texts, train_labels, val_labels = train_test_split(
        texts, labels, test_size=0.2, random_state=42, stratify=labels
    )

    tokenizer = BertTokenizer.from_pretrained(config.pretrained_model_name)

    train_dataset = MedicalDataset(train_texts, train_labels, tokenizer, config.max_seq_length)
    val_dataset = MedicalDataset(val_texts, val_labels, tokenizer, config.max_seq_length)

    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)

    model = MedicalAIModel(config, len(label_map))

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    optimizer = AdamW(model.parameters(), lr=config.learning_rate)
    total_steps = len(train_loader) * config.epochs
    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=config.warmup_steps,
        num_training_steps=total_steps
    )

    criterion = nn.CrossEntropyLoss()

    monitor = TrainingMonitor(config)

    for epoch in range(config.epochs):
        model.train()
        total_train_loss = 0
        train_predictions = []
        train_true_labels = []

        for batch in train_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            optimizer.zero_grad()

            outputs = model(input_ids, attention_mask)
            loss = criterion(outputs, labels)

            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)
            optimizer.step()
            scheduler.step()

            total_train_loss += loss.item()

            _, preds = torch.max(outputs, dim=1)
            train_predictions.extend(preds.cpu().tolist())
            train_true_labels.extend(labels.cpu().tolist())

        avg_train_loss = total_train_loss / len(train_loader)
        train_accuracy = accuracy_score(train_true_labels, train_predictions)

        model.eval()
        total_val_loss = 0
        val_predictions = []
        val_true_labels = []

        with torch.no_grad():
            for batch in val_loader:
                input_ids = batch['input_ids'].to(device)
                attention_mask = batch['attention_mask'].to(device)
                labels = batch['labels'].to(device)

                outputs = model(input_ids, attention_mask)
                loss = criterion(outputs, labels)

                total_val_loss += loss.item()

                _, preds = torch.max(outputs, dim=1)
                val_predictions.extend(preds.cpu().tolist())
                val_true_labels.extend(labels.cpu().tolist())

        avg_val_loss = total_val_loss / len(val_loader)
        val_accuracy = accuracy_score(val_true_labels, val_predictions)

        current_lr = scheduler.get_last_lr()[0]
        monitor.update(avg_train_loss, avg_val_loss, train_accuracy, val_accuracy, current_lr)

        logger.info(f"Epoch {epoch + 1}/{config.epochs} - "
                    f"Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, "
                    f"Train Acc: {train_accuracy:.4f}, Val Acc: {val_accuracy:.4f}")

    # ä¿å­˜æ‰€æœ‰å›¾è¡¨
    learning_curve_path = monitor.plot_learning_curves()
    resource_usage_path = monitor.plot_resource_usage()

    logger.info(f"å­¦ä¹ æ›²çº¿å·²ä¿å­˜: {learning_curve_path}")
    logger.info(f"èµ„æºä½¿ç”¨å›¾å·²ä¿å­˜: {resource_usage_path}")

    model_path = os.path.join(config.model_dir, "diagnosis_model.pth")
    torch.save(model.state_dict(), model_path)
    logger.info(f"æ¨¡å‹å·²ä¿å­˜: {model_path}")

    return model, tokenizer, label_map, monitor


# ========== ä¸»å‡½æ•° ==========
async def main():
    try:
        config = MedicalConfig()

        logger.info("\n[1/4] åŠ è½½åŒ»ç–—çŸ¥è¯†...")
        knowledge_base = MedicalKnowledgeBase(config)

        # éªŒè¯çŸ¥è¯†å›¾è°±
        validation_results = knowledge_base.validate_knowledge_graph()
        logger.info("çŸ¥è¯†å›¾è°±éªŒè¯ç»“æœ:")
        for result in validation_results:
            logger.info(f"  - {result}")

        logger.info("\n[2/4] å‡†å¤‡è®­ç»ƒæ•°æ®...")
        texts, labels, label_map = knowledge_base.prepare_training_data()

        if len(texts) == 0:
            logger.error("æ²¡æœ‰è¶³å¤Ÿçš„è®­ç»ƒæ•°æ®ï¼Œè¯·ç¡®ä¿çŸ¥è¯†åº“ä¸­æœ‰è¶³å¤Ÿçš„åŒ»ç–—ä¿¡æ¯")
            return

        logger.info("\n[3/4] è®­ç»ƒåŒ»ç–—è¯Šæ–­æ¨¡å‹...")
        model, tokenizer, label_map, monitor = await train_model(config, knowledge_base)

        if model is None:
            logger.error("æ¨¡å‹è®­ç»ƒå¤±è´¥")
            return

        logger.info("\n[4/4] å¯åŠ¨åŒ»ç–—åŠ©æ‰‹")
        assistant = MedicalAssistant(knowledge_base, config, model, tokenizer, label_map)

        logger.info("\n=== åŒ»ç–—è¯Šæ–­åŠ©æ‰‹ (åŒ»ç”Ÿç‰ˆ) ===")
        logger.info("è¾“å…¥æ‚£è€…ç—‡çŠ¶è¿›è¡Œè¯Šæ–­æˆ–è¾“å…¥'exit'é€€å‡º")
        logger.info(f"è¯Šæ–­é˜ˆå€¼: {config.diagnosis_threshold * 100}%")
        logger.info("æ”¯æŒä¸­è‹±æ–‡è¾“å…¥")

        while True:
            user_input = input("\nè¾“å…¥ç—‡çŠ¶: ").strip()

            if user_input.lower() == "exit":
                break

            response = await assistant.diagnose(user_input)
            print(f"\n{response}")

    except Exception as e:
        error_msg = f"ç³»ç»Ÿé”™è¯¯: {str(e)}"
        logger.error(error_msg)
        print(error_msg)


if __name__ == "__main__":
    asyncio.run(main())
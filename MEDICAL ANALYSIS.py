import os
import re
import json
import logging
import difflib
import numpy as np
import pandas as pd
import csv
from datetime import datetime
from difflib import SequenceMatcher
import asyncio
from deep_translator import GoogleTranslator
import Levenshtein
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import psutil
import GPUtil
import networkx as nx
from collections import deque
import torch
import torch.nn as nn
from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup
from torch.utils.data import Dataset, DataLoader
import warnings

warnings.filterwarnings('ignore')


# ========== LOGGER SETUP ==========
def setup_logger():
    """è®¾ç½®æ—¥å¿—è®°å½•å™¨ (Set up logger)"""
    log_dir = "logs"
    os.makedirs(log_dir, exist_ok=True)

    # åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„æ—¥å¿—æ–‡ä»¶
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = os.path.join(log_dir, f"medical_diagnosis_{timestamp}.log")

    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        filename=log_file,
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        encoding='utf-8'
    )

    # æ·»åŠ æ§åˆ¶å°è¾“å‡º
    console = logging.StreamHandler()
    console.setLevel(logging.INFO)
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    console.setFormatter(formatter)
    logging.getLogger('').addHandler(console)

    return logging.getLogger('MedicalDiagnosis')


# åˆå§‹åŒ–æ—¥å¿—è®°å½•å™¨
logger = setup_logger()


# ========== é…ç½® ==========
class MedicalConfig:
    def __init__(self):
        # çŸ¥è¯†åº“è·¯å¾„
        self.knowledge_paths = self.setup_knowledge_paths()

        # æ¨¡å‹ç›®å½•
        self.model_dir = "trained_models"
        os.makedirs(self.model_dir, exist_ok=True)

        # å¯è§†åŒ–ç›®å½•
        self.viz_dir = "visualizations"
        os.makedirs(self.viz_dir, exist_ok=True)

        # çŸ¥è¯†å›¾è°±ç›®å½•
        self.kg_dir = "knowledge_graphs"
        os.makedirs(self.kg_dir, exist_ok=True)

        logger.info(f"æ¨¡å‹ç›®å½• | Model directory: {self.model_dir}")
        logger.info(f"å¯è§†åŒ–ç›®å½• | Visualization directory: {self.viz_dir}")

        # è®­ç»ƒé…ç½®
        self.epochs = 10
        self.batch_size = 8
        self.learning_rate = 2e-5
        self.ddd_threshold = 1.0  # DDDé«˜é˜ˆå€¼
        self.warmup_steps = 100
        self.max_grad_norm = 1.0

        # è¯Šæ–­é˜ˆå€¼è®¾ä¸º95%
        self.diagnosis_threshold = 0.95

        # çŸ¥è¯†å›¾è°±é…ç½®
        self.kg_relation_types = [
            "has_symptom", "treated_with", "diagnosed_by",
            "symptom_of", "causes", "prevents", "contraindicates"
        ]

        # æ¨¡å‹é…ç½®
        self.pretrained_model_name = "bert-base-uncased"
        self.hidden_dropout_prob = 0.3
        self.num_labels = 10  # å‡è®¾æœ‰10ç§ä¸»è¦ç–¾ç—…ç±»å‹

        # æ€è€ƒæ·±åº¦é…ç½®
        self.thinking_depth = 3  # æ¨ç†æ·±åº¦
        self.certainty_threshold = 0.8  # ç¡®å®šæ€§é˜ˆå€¼

        logger.info("\n=== åŒ»ç–—åˆ†æé…ç½® | MEDICAL ANALYSIS CONFIGURATION ===")
        logger.info(f"çŸ¥è¯†è·¯å¾„ | Knowledge Paths: {self.knowledge_paths}")
        logger.info(f"è¯Šæ–­é˜ˆå€¼ | Diagnosis Threshold: {self.diagnosis_threshold * 100}%")
        logger.info(f"è®­ç»ƒè½®æ•° | Training Epochs: {self.epochs}")
        logger.info(f"æ€è€ƒæ·±åº¦ | Thinking Depth: {self.thinking_depth}")
        logger.info("===================================================")

    def setup_knowledge_paths(self):
        """è®¾ç½®çŸ¥è¯†åº“è·¯å¾„ä¸º'knowledge_base'æ–‡ä»¶å¤¹ (Set knowledge base paths)"""
        knowledge_dir = os.path.join(os.getcwd(), "knowledge_base")
        os.makedirs(knowledge_dir, exist_ok=True)
        logger.info(f"çŸ¥è¯†è·¯å¾„ | Knowledge path: {knowledge_dir}")
        return [knowledge_dir]

    async def translate_to_english(self, text):
        """å¼‚æ­¥ç¿»è¯‘æ–‡æœ¬åˆ°è‹±æ–‡ (Translate text to English asynchronously)"""
        try:
            # ä½¿ç”¨ to_thread è¿è¡ŒåŒæ­¥ç¿»è¯‘ä»»åŠ¡
            return await asyncio.to_thread(GoogleTranslator(source='auto', target='en').translate, text)
        except Exception as e:
            logger.error(f"ç¿»è¯‘é”™è¯¯ | Translation error: {str(e)}")
            return text

    async def translate_to_chinese(self, text):
        """å¼‚æ­¥ç¿»è¯‘æ–‡æœ¬åˆ°ä¸­æ–‡ (Translate text to Chinese asynchronously)"""
        try:
            # ä½¿ç”¨ asyncio.to_thread æ¥è¿è¡ŒåŒæ­¥ç¿»è¯‘ä»»åŠ¡
            return await asyncio.to_thread(GoogleTranslator(source='auto', target='zh-CN').translate, text)
        except Exception as e:
            logger.error(f"ç¿»è¯‘é”™è¯¯ | Translation error: {str(e)}")
            return text

    def is_english(self, text):
        """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦ä¸ºè‹±æ–‡ (Check if text is English)"""
        try:
            text.encode('ascii')
            return True
        except:
            return False

    def is_chinese(self, text):
        """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦ä¸ºä¸­æ–‡ (Check if text is Chinese)"""
        return any('\u4e00' <= char <= '\u9fff' for char in text)

    async def translate_bilingual(self, en_text, cn_text):
        """åˆ›å»ºåŒè¯­æ–‡æœ¬ (Create bilingual text)"""
        return f"ğŸŒ ENGLISH:\n{en_text}\n\nğŸŒ ä¸­æ–‡:\n{cn_text}"


# ========== çŸ¥è¯†å›¾è°±ç³»ç»Ÿ ==========
class MedicalKnowledgeGraph:
    def __init__(self, config):
        self.config = config
        self.graph = nx.MultiDiGraph()
        self.entity_types = ["disease", "symptom", "medication", "test", "anatomy"]
        self.relation_counter = {rel: 0 for rel in self.config.kg_relation_types}

    def add_entity(self, entity_id, entity_type, properties=None):
        """æ·»åŠ å®ä½“åˆ°çŸ¥è¯†å›¾è°±"""
        if entity_type not in self.entity_types:
            logger.warning(f"æœªçŸ¥å®ä½“ç±»å‹: {entity_type}")
            return False

        if properties is None:
            properties = {}

        properties['type'] = entity_type
        self.graph.add_node(entity_id, **properties)
        return True

    def add_relation(self, source_id, target_id, relation_type, properties=None):
        """æ·»åŠ å…³ç³»åˆ°çŸ¥è¯†å›¾è°±"""
        if relation_type not in self.config.kg_relation_types:
            logger.warning(f"æœªçŸ¥å…³ç³»ç±»å‹: {relation_type}")
            return False

        if properties is None:
            properties = {}

        self.graph.add_edge(source_id, target_id, key=relation_type, **properties)
        self.relation_counter[relation_type] += 1
        return True

    def find_entities(self, entity_name, entity_type=None):
        """æ ¹æ®åç§°æŸ¥æ‰¾å®ä½“"""
        results = []
        for node, data in self.graph.nodes(data=True):
            if 'name' in data and data['name'] == entity_name:
                if entity_type is None or data.get('type') == entity_type:
                    results.append((node, data))
        return results

    def find_related_entities(self, entity_id, relation_type=None, max_depth=1):
        """æŸ¥æ‰¾ç›¸å…³å®ä½“"""
        related_entities = set()

        # å¹¿åº¦ä¼˜å…ˆæœç´¢
        queue = deque([(entity_id, 0)])
        visited = set([entity_id])

        while queue:
            current_id, depth = queue.popleft()

            if depth > max_depth:
                continue

            # è·å–æ‰€æœ‰å‡ºè¾¹å’Œå…¥è¾¹
            for _, neighbor, key, data in self.graph.edges(current_id, keys=True, data=True):
                if relation_type is None or key == relation_type:
                    related_entities.add(neighbor)

                if neighbor not in visited and depth < max_depth:
                    visited.add(neighbor)
                    queue.append((neighbor, depth + 1))

            for neighbor, _, key, data in self.graph.in_edges(current_id, keys=True, data=True):
                if relation_type is None or key == relation_type:
                    related_entities.add(neighbor)

                if neighbor not in visited and depth < max_depth:
                    visited.add(neighbor)
                    queue.append((neighbor, depth + 1))

        return [(entity, self.graph.nodes[entity]) for entity in related_entities]

    def visualize(self, filename="knowledge_graph.png"):
        """å¯è§†åŒ–çŸ¥è¯†å›¾è°±"""
        plt.figure(figsize=(20, 15))

        # æ ¹æ®å®ä½“ç±»å‹è®¾ç½®é¢œè‰²
        node_colors = []
        for node in self.graph.nodes():
            node_type = self.graph.nodes[node].get('type', 'unknown')
            if node_type == "disease":
                node_colors.append('red')
            elif node_type == "symptom":
                node_colors.append('blue')
            elif node_type == "medication":
                node_colors.append('green')
            elif node_type == "test":
                node_colors.append('orange')
            elif node_type == "anatomy":
                node_colors.append('purple')
            else:
                node_colors.append('gray')

        # ç»˜åˆ¶çŸ¥è¯†å›¾è°±
        pos = nx.spring_layout(self.graph, k=1, iterations=50)
        nx.draw_networkx_nodes(self.graph, pos, node_color=node_colors, node_size=500, alpha=0.8)

        # ç»˜åˆ¶è¾¹
        edge_colors = []
        for u, v, key in self.graph.edges(keys=True):
            if key == "has_symptom":
                edge_colors.append('blue')
            elif key == "treated_with":
                edge_colors.append('green')
            elif key == "diagnosed_by":
                edge_colors.append('orange')
            elif key == "symptom_of":
                edge_colors.append('lightblue')
            elif key == "causes":
                edge_colors.append('red')
            elif key == "prevents":
                edge_colors.append('lightgreen')
            elif key == "contraindicates":
                edge_colors.append('pink')
            else:
                edge_colors.append('gray')

        nx.draw_networkx_edges(self.graph, pos, edge_color=edge_colors, alpha=0.5)

        # æ·»åŠ æ ‡ç­¾
        labels = {}
        for node in self.graph.nodes():
            labels[node] = self.graph.nodes[node].get('name', node)[:15]  # é™åˆ¶æ ‡ç­¾é•¿åº¦

        nx.draw_networkx_labels(self.graph, pos, labels, font_size=8)

        # æ·»åŠ å…³ç³»ç±»å‹æ ‡ç­¾
        edge_labels = {}
        for u, v, key in self.graph.edges(keys=True):
            edge_labels[(u, v)] = key

        nx.draw_networkx_edge_labels(self.graph, pos, edge_labels, font_size=6)

        plt.title("Medical Knowledge Graph")
        plt.axis('off')
        plt.tight_layout()

        # ä¿å­˜å›¾åƒ
        filepath = os.path.join(self.config.kg_dir, filename)
        plt.savefig(filepath, dpi=300, bbox_inches='tight')
        plt.close()

        logger.info(f"çŸ¥è¯†å›¾è°±å·²ä¿å­˜: {filepath}")
        return filepath

    def export_to_json(self, filename="knowledge_graph.json"):
        """å¯¼å‡ºçŸ¥è¯†å›¾è°±åˆ°JSONæ–‡ä»¶"""
        data = {
            "nodes": [],
            "edges": []
        }

        # æ·»åŠ èŠ‚ç‚¹
        for node, node_data in self.graph.nodes(data=True):
            node_data["id"] = node
            data["nodes"].append(node_data)

        # æ·»åŠ è¾¹
        for u, v, key, edge_data in self.graph.edges(data=True, keys=True):
            edge_data.update({
                "source": u,
                "target": v,
                "type": key
            })
            data["edges"].append(edge_data)

        # ä¿å­˜åˆ°æ–‡ä»¶
        filepath = os.path.join(self.config.kg_dir, filename)
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        logger.info(f"çŸ¥è¯†å›¾è°±å·²å¯¼å‡º: {filepath}")
        return filepath


# ========== çŸ¥è¯†åº“ ==========
class MedicalKnowledgeBase:
    def __init__(self, config):
        self.config = config
        self.disease_info = {}
        self.symptom_info = {}
        self.medication_ddd_info = {}
        self.full_knowledge = []  # å­˜å‚¨å®Œæ•´çš„çŸ¥è¯†åº“å†…å®¹
        self.knowledge_graph = MedicalKnowledgeGraph(config)
        self.learning_stats = {
            "files_processed": 0,
            "diseases_extracted": 0,
            "symptoms_extracted": 0,
            "medications_extracted": 0,
            "tests_extracted": 0,
            "total_size_kb": 0,
            "kg_entities": 0,
            "kg_relations": 0
        }

        # åŠ è½½çŸ¥è¯†
        self.load_knowledge()
        logger.info(f"çŸ¥è¯†åº“åŠ è½½å®Œæˆ | Knowledge base loaded: "
                    f"{len(self.disease_info)}ç§ç–¾ç—… | diseases, "
                    f"{len(self.symptom_info)}ç§ç—‡çŠ¶ | symptoms, "
                    f"{self.learning_stats['files_processed']}ä¸ªæ–‡ä»¶ | files, "
                    f"{self.learning_stats['total_size_kb']:.2f} KBå†…å®¹ | content, "
                    f"{self.learning_stats['kg_entities']}ä¸ªçŸ¥è¯†å›¾è°±å®ä½“ | KG entities, "
                    f"{self.learning_stats['kg_relations']}ä¸ªçŸ¥è¯†å›¾è°±å…³ç³» | KG relations")

    def extract_medical_info(self, text, file_path):
        """ä»æ–‡æœ¬ä¸­æå–åŒ»ç–—ä¿¡æ¯ (æ”¯æŒå¤šè¯­è¨€) (Extract medical info from text)"""
        try:
            # ä¿å­˜å®Œæ•´çŸ¥è¯†
            self.full_knowledge.append({
                "file_path": file_path,
                "content": text,
                "size_kb": len(text.encode('utf-8')) / 1024
            })
            self.learning_stats["total_size_kb"] += len(text.encode('utf-8')) / 1024

            # ç–¾ç—…æå– (æ”¯æŒä¸­è‹±æ–‡)
            disease_pattern = r'(?:disease|condition|illness|diagnosis|ç–¾ç—…|ç—…ç—‡|è¯Šæ–­)[\s:ï¼š]*([^\n]+)'
            disease_matches = re.findall(disease_pattern, text, re.IGNORECASE)

            for match in disease_matches:
                disease_name = match.strip().split('\n')[0].split(',')[0].strip()

                # æ·»åŠ åˆ°çŸ¥è¯†å›¾è°±
                disease_id = f"disease_{len(self.disease_info)}"
                self.knowledge_graph.add_entity(disease_id, "disease", {"name": disease_name})
                self.learning_stats["kg_entities"] += 1

                # ç—‡çŠ¶æå– (æ”¯æŒä¸­è‹±æ–‡)
                symptoms = []
                symptom_pattern = r'(?:symptoms|signs|complaint|ç—‡çŠ¶|ä½“å¾|ä¸é€‚)[\s:ï¼š]*([^\n]+)'
                symptom_matches = re.findall(symptom_pattern, text, re.IGNORECASE)
                for sm in symptom_matches:
                    symptoms.extend([s.strip() for s in re.split(r'[,ï¼Œã€]', sm)])

                    # æ·»åŠ åˆ°çŸ¥è¯†å›¾è°±
                    for symptom in symptoms:
                        symptom_id = f"symptom_{len(self.symptom_info)}"
                        self.knowledge_graph.add_entity(symptom_id, "symptom", {"name": symptom})
                        self.knowledge_graph.add_relation(disease_id, symptom_id, "has_symptom")
                        self.knowledge_graph.add_relation(symptom_id, disease_id, "symptom_of")
                        self.learning_stats["kg_entities"] += 1
                        self.learning_stats["kg_relations"] += 2

                # è¯ç‰©æå– (æ”¯æŒä¸­è‹±æ–‡) - åŒ…å«è§„æ ¼å’ŒDDDå€¼
                medications = []
                medication_pattern = r'(?:medications|drugs|prescriptions|å‰‚é‡|è¯ç‰©)[\s:ï¼š]*([^\n]+)'
                medication_matches = re.findall(medication_pattern, text, re.IGNORECASE)

                for mm in medication_matches:
                    for line in mm.split('\n'):
                        # æ”¯æŒå¤šç§æ ¼å¼çš„è¯ç‰©æè¿°ï¼ŒåŒ…æ‹¬è§„æ ¼å’ŒDDDå€¼
                        # åŒ¹é…æ ¼å¼: è¯ç‰©åç§° è§„æ ¼ (å¦‚: 10mg/ç‰‡) DDD:å€¼
                        med_match = re.search(
                            r'([a-zA-Z\u4e00-\u9fff]+[\s\-]*[a-zA-Z\u4e00-\u9fff]*\d*)[\s(]*([\d.]+[a-zA-Z\u4e00-\u9fff/]+)\s*(?:DDD:?\s*([\d.]+))?',
                            line, re.IGNORECASE)
                        if med_match:
                            name = med_match.group(1).strip()
                            specification = med_match.group(2).strip() if med_match.group(2) else ""
                            ddd_value = float(med_match.group(3)) if med_match.group(3) else None

                            medications.append({
                                'name': name,
                                'specification': specification,
                                'ddd': ddd_value
                            })

                            # æ·»åŠ åˆ°çŸ¥è¯†å›¾è°±
                            med_id = f"medication_{len(medications)}"
                            self.knowledge_graph.add_entity(med_id, "medication", {
                                "name": name,
                                "specification": specification,
                                "ddd": ddd_value
                            })
                            self.knowledge_graph.add_relation(disease_id, med_id, "treated_with")
                            self.learning_stats["kg_entities"] += 1
                            self.learning_stats["kg_relations"] += 1

                # æ£€æŸ¥æå– (æ”¯æŒä¸­è‹±æ–‡)
                tests = []
                test_pattern = r'(?:tests|examinations|diagnostic procedures|æ£€æŸ¥|æ£€éªŒ|æ£€æµ‹)[\s:ï¼š]*([^\n]+)'
                test_matches = re.findall(test_pattern, text, re.IGNORECASE)
                for tm in test_matches:
                    tests.extend([t.strip() for t in re.split(r'[,ï¼Œã€]', tm)])

                    # æ·»åŠ åˆ°çŸ¥è¯†å›¾è°±
                    for test in tests:
                        test_id = f"test_{len(tests)}"
                        self.knowledge_graph.add_entity(test_id, "test", {"name": test})
                        self.knowledge_graph.add_relation(disease_id, test_id, "diagnosed_by")
                        self.learning_stats["kg_entities"] += 1
                        self.learning_stats["kg_relations"] += 1

                # ä¿å­˜ç–¾ç—…ä¿¡æ¯
                if disease_name and disease_name not in self.disease_info:
                    self.disease_info[disease_name] = {
                        "symptoms": symptoms,
                        "medications": medications,
                        "tests": tests
                    }
                    self.learning_stats["diseases_extracted"] += 1
                    self.learning_stats["medications_extracted"] += len(medications)
                    self.learning_stats["tests_extracted"] += len(tests)

                    # å­˜å‚¨è¯ç‰©DDDä¿¡æ¯
                    for med in medications:
                        if med['ddd'] is not None:
                            self.medication_ddd_info[med['name']] = med['ddd']

            # æå–ç—‡çŠ¶ä¿¡æ¯
            symptom_names = set()
            for symptom_list in [info["symptoms"] for info in self.disease_info.values()]:
                symptom_names.update(symptom_list)

            for symptom in symptom_names:
                if symptom and symptom not in self.symptom_info:
                    self.symptom_info[symptom] = {
                        "description": "",
                        "related_tests": []
                    }
                    self.learning_stats["symptoms_extracted"] += 1

            return True
        except Exception as e:
            logger.error(f"åŒ»ç–—ä¿¡æ¯æå–é”™è¯¯ | Medical info extraction error: {str(e)}")
            return False

    async def calculate_ddd(self, medication, specification):
        """è®¡ç®—DDDå€¼ (Calculate DDD value)"""
        # 1. é¦–å…ˆæ£€æŸ¥çŸ¥è¯†åº“ä¸­æ˜¯å¦æœ‰è¯¥è¯ç‰©çš„DDDä¿¡æ¯
        if medication in self.medication_ddd_info:
            ddd_value = self.medication_ddd_info[medication]
            return ddd_value, None

        # 2. çŸ¥è¯†åº“ä¸­æ²¡æœ‰åˆ™å°è¯•å¯»æ‰¾æ›¿ä»£è¯ç‰©
        alternatives = await self.find_alternative_medications(medication)
        if alternatives:
            return None, alternatives  # è¿”å›Noneè¡¨ç¤ºéœ€è¦æ¢è¯

        # 3. æœ€åå°è¯•é¢„æµ‹DDD
        ddd_value = self.predict_ddd_with_model(medication, specification)
        if ddd_value is not None:
            return ddd_value, None
        else:
            return None, ["çŸ¥è¯†åº“ä¸­æ²¡æœ‰DDDå€¼ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ›´æ–°çŸ¥è¯†åº“"]  # è¿”å›é”™è¯¯æ¶ˆæ¯

    async def find_alternative_medications(self, medication):
        """åœ¨çŸ¥è¯†åº“ä¸­å¯»æ‰¾æ›¿ä»£è¯ç‰© (Find alternative medications in knowledge base)"""
        alternatives = []
        for disease, info in self.disease_info.items():
            for med in info.get("medications", []):
                med_name = med["name"]
                # ç›¸ä¼¼è¯ç‰©åŒ¹é… (æ”¯æŒå¤šè¯­è¨€)
                if await self.is_similar_medication(medication, med_name) and med_name != medication:
                    alternatives.append({
                        "name": med_name,
                        "specification": med.get("specification", "")
                    })
        return alternatives  # è¿”å›æ›¿ä»£è¯ç‰©åˆ—è¡¨

    async def is_similar_medication(self, med1, med2):
        """æ£€æŸ¥è¯ç‰©æ˜¯å¦ç›¸ä¼¼ (æ”¯æŒå¤šè¯­è¨€) (Check if medications are similar)"""
        # ç¿»è¯‘ä¸ºè‹±æ–‡åæ¯”è¾ƒ
        med1_en = await self.config.translate_to_english(med1)
        med2_en = await self.config.translate_to_english(med2)

        # æ£€æŸ¥ç¿»è¯‘ç»“æœæ˜¯å¦ä¸º None æˆ–ç©ºå­—ç¬¦ä¸²ï¼Œå¹¶è¿›è¡Œå°å†™è½¬æ¢
        med1_en_lower = (med1_en or "").lower()
        med2_en_lower = (med2_en or "").lower()

        if not med1_en_lower or not med2_en_lower:
            return False  # å¦‚æœç¿»è¯‘å¤±è´¥ï¼Œè®¤ä¸ºä¸ç›¸ä¼¼

        return SequenceMatcher(None, med1_en_lower, med2_en_lower).ratio() > 0.7

    def predict_ddd_with_model(self, medication, specification):
        """ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹é¢„æµ‹DDDå€¼ (Predict DDD using trained model)"""
        # è¿™é‡Œåº”è¯¥åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹
        # ç®€åŒ–ç‰ˆï¼šè¿”å›å›ºå®šå€¼æˆ–åŸºäºè§„åˆ™çš„é¢„æµ‹
        try:
            # å°è¯•ä»æ¨¡å‹ç›®å½•åŠ è½½æ¨¡å‹
            model_path = os.path.join(self.config.model_dir, "ddd_predictor.model")
            if os.path.exists(model_path):
                # å®é™…åº”ç”¨ä¸­åº”è¯¥åŠ è½½æ¨¡å‹å¹¶è¿›è¡Œé¢„æµ‹
                # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œè¿”å›ä¸€ä¸ªåŸºäºåç§°å’Œè§„æ ¼çš„ç®€å•é¢„æµ‹
                if "ç¡è‹¯" in medication or "nifedipine" in medication.lower():
                    return 10.0
                elif "æ°¨æ°¯" in medication or "amlodipine" in medication.lower():
                    return 5.0
                elif "å„è´" in medication or "irbesartan" in medication.lower():
                    return 150.0
                else:
                    # é»˜è®¤è¿”å›ä¸€ä¸ªåŸºäºè§„æ ¼çš„ä¼°è®¡å€¼
                    try:
                        # å°è¯•ä»è§„æ ¼ä¸­æå–æ•°å­—
                        numbers = re.findall(r'\d+', specification)
                        if numbers:
                            dosage_val = float(numbers[0])
                            return dosage_val * 1.5  # ç®€å•ä¼°ç®—
                    except:
                        return 10.0  # é»˜è®¤å€¼
            else:
                logger.warning("æœªæ‰¾åˆ°è®­ç»ƒå¥½çš„DDDé¢„æµ‹æ¨¡å‹ | DDD prediction model not found")
                return None
        except Exception as e:
            logger.error(f"DDDé¢„æµ‹é”™è¯¯ | DDD prediction error: {str(e)}")
            return None

    def load_knowledge(self):
        """ä»çŸ¥è¯†åº“æ–‡ä»¶å¤¹åŠ è½½æ‰€æœ‰çŸ¥è¯†åº“æ–‡ä»¶ (Load all knowledge base files)"""
        logger.info("ä»æœ¬åœ°çŸ¥è¯†åº“æ–‡ä»¶å¤¹åŠ è½½åŒ»ç–—çŸ¥è¯† | Loading medical knowledge from local knowledge_base folder...")

        for path in self.config.knowledge_paths:
            if not os.path.exists(path):
                logger.warning(f"çŸ¥è¯†è·¯å¾„æœªæ‰¾åˆ° | Knowledge path not found: {path}")
                continue

            logger.info(f"å¤„ç†ç›®å½• | Processing directory: {path}")
            file_count = 0

            for root, _, files in os.walk(path):
                for file in files:
                    file_path = os.path.join(root, file)
                    if any(file_path.endswith(ext) for ext in ('.txt', '.csv', '.json', '.docx', '.pdf')):
                        logger.info(f"å¤„ç†æ–‡ä»¶ | Processing file: {file_path}")
                        try:
                            # åŠ è½½æ–‡ä»¶å†…å®¹
                            content = self.load_file(file_path)

                            # æå–åŒ»ç–—ä¿¡æ¯
                            self.extract_medical_info(content, file_path)

                            file_count += 1
                            self.learning_stats["files_processed"] += 1
                        except Exception as e:
                            logger.error(f"æ–‡ä»¶å¤„ç†é”™è¯¯ | Error processing file {file_path}: {str(e)}")

            logger.info(f"åœ¨è·¯å¾„ä¸­å¤„ç†æ–‡ä»¶æ•° | Processed {file_count} files in {path}")

        # å¯è§†åŒ–çŸ¥è¯†å›¾è°±
        if self.learning_stats["kg_entities"] > 0:
            self.knowledge_graph.visualize()
            self.knowledge_graph.export_to_json()

        # å®Œå…¨ç§»é™¤ä»»ä½•é»˜è®¤çŸ¥è¯†æ·»åŠ 
        if not self.disease_info:
            logger.warning("çŸ¥è¯†åº“æ–‡ä»¶ä¸­æœªæå–åˆ°ç–¾ç—… | No diseases extracted from knowledge base files")
        if not self.symptom_info:
            logger.warning("çŸ¥è¯†åº“æ–‡ä»¶ä¸­æœªæå–åˆ°ç—‡çŠ¶ | No symptoms extracted from knowledge base files")
        if not self.full_knowledge:
            logger.warning("çŸ¥è¯†åº“æœªåŠ è½½ä»»ä½•å†…å®¹ | No content loaded in knowledge base")

    def load_file(self, file_path):
        """åŠ è½½å•ä¸ªçŸ¥è¯†æ–‡ä»¶ (Load a single knowledge file)"""
        try:
            content = ""
            if file_path.endswith('.txt'):
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
            elif file_path.endswith('.csv'):
                with open(file_path, 'r', encoding='utf-8') as f:
                    reader = csv.reader(f)
                    content = "\n".join([",".join(row) for row in reader])
            elif file_path.endswith('.json'):
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    content = json.dumps(data, ensure_ascii=False)
            elif file_path.endswith('.docx'):
                import docx
                doc = docx.Document(file_path)
                content = "\n".join([para.text for para in doc.paragraphs])
            elif file_path.endswith('.pdf'):
                from PyPDF2 import PdfReader
                with open(file_path, 'rb') as f:
                    pdf_reader = PdfReader(f)
                    for page in pdf_reader.pages:
                        content += page.extract_text() + "\n"
            else:
                logger.warning(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ | Unsupported file format: {file_path}")
                return ""

            return content
        except Exception as e:
            logger.error(f"æ–‡ä»¶åŠ è½½é”™è¯¯ | Error loading file {file_path}: {str(e)}")
            return ""


# ========== åŒ»ç–—AIæ¨¡å‹ ==========
class MedicalAIModel(nn.Module):
    def __init__(self, config):
        super(MedicalAIModel, self).__init__()
        self.config = config
        self.bert = BertModel.from_pretrained(config.pretrained_model_name)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)
        self.classifier = nn.Linear(self.bert.config.hidden_size, config.num_labels)
        self.attention_weights = None

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs.pooler_output
        pooled_output = self.dropout(pooled_output)

        # è·å–æ³¨æ„åŠ›æƒé‡ç”¨äºå¯è§†åŒ–
        self.attention_weights = outputs.last_hidden_state

        logits = self.classifier(pooled_output)
        return logits


# ========== è®­ç»ƒç›‘æ§å™¨ ==========
class TrainingMonitor:
    def __init__(self, config):
        self.config = config
        self.train_losses = []
        self.val_losses = []
        self.train_accuracies = []
        self.val_accuracies = []
        self.learning_rates = []
        self.cpu_usages = []
        self.gpu_usages = []
        self.memory_usages = []
        self.timestamps = []

    def update(self, train_loss, val_loss, train_acc, val_acc, lr):
        """æ›´æ–°è®­ç»ƒæŒ‡æ ‡"""
        self.train_losses.append(train_loss)
        self.val_losses.append(val_loss)
        self.train_accuracies.append(train_acc)
        self.val_accuracies.append(val_acc)
        self.learning_rates.append(lr)
        self.cpu_usages.append(psutil.cpu_percent())

        # è·å–GPUä½¿ç”¨æƒ…å†µ
        gpus = GPUtil.getGPUs()
        if gpus:
            self.gpu_usages.append(gpus[0].load * 100)
        else:
            self.gpu_usages.append(0)

        self.memory_usages.append(psutil.virtual_memory().percent)
        self.timestamps.append(datetime.now())

    def plot_learning_curves(self, filename="learning_curves.png"):
        """ç»˜åˆ¶å­¦ä¹ æ›²çº¿"""
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))

        # æŸå¤±æ›²çº¿
        ax1.plot(self.train_losses, label='Training Loss')
        ax1.plot(self.val_losses, label='Validation Loss')
        ax1.set_title('Training and Validation Loss')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.legend()
        ax1.grid(True)

        # å‡†ç¡®ç‡æ›²çº¿
        ax2.plot(self.train_accuracies, label='Training Accuracy')
        ax2.plot(self.val_accuracies, label='Validation Accuracy')
        ax2.set_title('Training and Validation Accuracy')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Accuracy')
        ax2.legend()
        ax2.grid(True)

        # å­¦ä¹ ç‡æ›²çº¿
        ax3.plot(self.learning_rates, label='Learning Rate', color='green')
        ax3.set_title('Learning Rate Schedule')
        ax3.set_xlabel('Epoch')
        ax3.set_ylabel('Learning Rate')
        ax3.legend()
        ax3.grid(True)

        # èµ„æºä½¿ç”¨æƒ…å†µ
        ax4.plot(self.cpu_usages, label='CPU Usage', color='red')
        ax4.plot(self.gpu_usages, label='GPU Usage', color='blue')
        ax4.plot(self.memory_usages, label='Memory Usage', color='green')
        ax4.set_title('Resource Usage')
        ax4.set_xlabel('Epoch')
        ax4.set_ylabel('Usage (%)')
        ax4.legend()
        ax4.grid(True)

        plt.tight_layout()

        # ä¿å­˜å›¾åƒ
        filepath = os.path.join(self.config.viz_dir, filename)
        plt.savefig(filepath, dpi=300, bbox_inches='tight')
        plt.close()

        logger.info(f"å­¦ä¹ æ›²çº¿å·²ä¿å­˜: {filepath}")
        return filepath

    def plot_attention_heatmap(self, attention_weights, tokens, filename="attention_heatmap.png"):
        """ç»˜åˆ¶æ³¨æ„åŠ›çƒ­åŠ›å›¾"""
        plt.figure(figsize=(12, 8))

        # å–æœ€åä¸€å±‚çš„æ³¨æ„åŠ›æƒé‡å¹³å‡å€¼
        avg_attention = attention_weights.mean(dim=1).squeeze().cpu().detach().numpy()

        # ç»˜åˆ¶çƒ­åŠ›å›¾
        sns.heatmap(avg_attention, xticklabels=tokens[:avg_attention.shape[1]],
                    yticklabels=[f"Head {i + 1}" for i in range(avg_attention.shape[0])],
                    cmap="YlOrRd")
        plt.title('Attention Heatmap')
        plt.xticks(rotation=45, ha='right')
        plt.tight_layout()

        # ä¿å­˜å›¾åƒ
        filepath = os.path.join(self.config.viz_dir, filename)
        plt.savefig(filepath, dpi=300, bbox_inches='tight')
        plt.close()

        logger.info(f"æ³¨æ„åŠ›çƒ­åŠ›å›¾å·²ä¿å­˜: {filepath}")
        return filepath

    def plot_thinking_process(self, thinking_steps, certainty_scores, filename="thinking_process.png"):
        """ç»˜åˆ¶æ€è€ƒè¿‡ç¨‹å›¾"""
        plt.figure(figsize=(12, 6))

        # åˆ›å»ºæ€è€ƒæ­¥éª¤çš„æ¡å½¢å›¾
        steps = [f"Step {i + 1}" for i in range(len(thinking_steps))]
        positions = np.arange(len(steps))

        plt.bar(positions, certainty_scores, color='skyblue', alpha=0.7)
        plt.plot(positions, certainty_scores, marker='o', color='red', linewidth=2)

        plt.xlabel('Thinking Steps')
        plt.ylabel('Certainty Score')
        plt.title('AI Thinking Process and Certainty Progression')
        plt.xticks(positions, steps)
        plt.ylim(0, 1)
        plt.grid(True, axis='y', alpha=0.3)

        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, v in enumerate(certainty_scores):
            plt.text(i, v + 0.02, f"{v:.2f}", ha='center', va='bottom')

        plt.tight_layout()

        # ä¿å­˜å›¾åƒ
        filepath = os.path.join(self.config.viz_dir, filename)
        plt.savefig(filepath, dpi=300, bbox_inches='tight')
        plt.close()

        logger.info(f"æ€è€ƒè¿‡ç¨‹å›¾å·²ä¿å­˜: {filepath}")
        return filepath


# ========== åŒ»ç–—åŠ©æ‰‹ ==========
class MedicalAssistant:
    def __init__(self, knowledge_base, config):
        self.knowledge_base = knowledge_base
        self.config = config
        self.thought_process = []  # è®°å½•æ€è€ƒè¿‡ç¨‹
        self.current_symptoms = []
        self.attempt_count = 0
        self.session_id = datetime.now().strftime("%Y%m%d%H%M%S")
        self.thinking_steps = []  # è®°å½•æ€è€ƒæ­¥éª¤
        self.certainty_scores = []  # è®°å½•ç¡®å®šæ€§åˆ†æ•°
        self.training_monitor = TrainingMonitor(config)

        # åŠ è½½è®­ç»ƒå¥½çš„è¯Šæ–­æ¨¡å‹
        self.diagnosis_model = self.load_diagnosis_model()

    def load_diagnosis_model(self):
        """åŠ è½½è®­ç»ƒå¥½çš„è¯Šæ–­æ¨¡å‹ (Load trained diagnosis model)"""
        try:
            model_path = os.path.join(self.config.model_dir, "diagnosis_model.pth")
            if os.path.exists(model_path):
                # åŠ è½½æ¨¡å‹
                model = MedicalAIModel(self.config)
                model.load_state_dict(torch.load(model_path))
                model.eval()
                logger.info("è¯Šæ–­æ¨¡å‹åŠ è½½æˆåŠŸ | Diagnosis model loaded successfully")
                return model
            else:
                logger.warning("æœªæ‰¾åˆ°è®­ç»ƒå¥½çš„è¯Šæ–­æ¨¡å‹ | Trained diagnosis model not found")
                return None
        except Exception as e:
            logger.error(f"æ¨¡å‹åŠ è½½é”™è¯¯ | Model loading error: {str(e)}")
            return None

    async def deep_think(self, symptoms, depth=0, max_depth=3, current_certainty=0.0):
        """æ·±åº¦æ€è€ƒè¿‡ç¨‹ï¼Œæ¨¡æ‹Ÿäººè„‘æ¨ç† (Deep thinking process simulating human reasoning)"""
        if depth >= max_depth:
            return current_certainty, symptoms

        # è®°å½•æ€è€ƒæ­¥éª¤
        step_info = {
            "depth": depth,
            "symptoms": symptoms.copy(),
            "certainty": current_certainty,
            "timestamp": datetime.now()
        }
        self.thinking_steps.append(step_info)
        self.certainty_scores.append(current_certainty)

        # ä½¿ç”¨çŸ¥è¯†å›¾è°±æ‰©å±•ç—‡çŠ¶
        expanded_symptoms = await self.expand_symptoms_with_kg(symptoms)

        # ä½¿ç”¨æ¨¡å‹è¿›è¡Œè¯Šæ–­
        diagnosis_result = await self.model_based_diagnosis(expanded_symptoms)

        # è®¡ç®—æ–°çš„ç¡®å®šæ€§
        new_certainty = diagnosis_result["confidence"]

        # å¦‚æœç¡®å®šæ€§è¶³å¤Ÿé«˜ï¼Œåœæ­¢é€’å½’
        if new_certainty >= self.config.certainty_threshold:
            return new_certainty, expanded_symptoms

        # å¦åˆ™ç»§ç»­æ·±å…¥æ€è€ƒ
        return await self.deep_think(expanded_symptoms, depth + 1, max_depth, new_certainty)

    async def expand_symptoms_with_kg(self, symptoms):
        """ä½¿ç”¨çŸ¥è¯†å›¾è°±æ‰©å±•ç—‡çŠ¶åˆ—è¡¨ (Expand symptoms using knowledge graph)"""
        expanded_symptoms = symptoms.copy()

        for symptom in symptoms:
            # åœ¨çŸ¥è¯†å›¾è°±ä¸­æŸ¥æ‰¾ç›¸å…³ç—‡çŠ¶
            symptom_entities = self.knowledge_base.knowledge_graph.find_entities(symptom, "symptom")

            for entity_id, entity_data in symptom_entities:
                # æŸ¥æ‰¾ç›¸å…³ç—‡çŠ¶ï¼ˆåŒä¸€ç§ç–¾ç—…çš„å…¶ä»–ç—‡çŠ¶ï¼‰
                related_entities = self.knowledge_base.knowledge_graph.find_related_entities(
                    entity_id, "symptom_of", max_depth=2
                )

                for related_id, related_data in related_entities:
                    if related_data.get('type') == 'symptom' and related_data.get('name') not in expanded_symptoms:
                        expanded_symptoms.append(related_data.get('name'))

        return expanded_symptoms

    async def diagnose(self, chief_complaint):
        """è¯Šæ–­æµç¨‹ (ç±»äººè„‘æ€è€ƒè¿‡ç¨‹) (Diagnosis process)"""
        self.thought_process = [f"æ‚£è€…ä¸»è¯‰ | Patient chief complaint: {chief_complaint}"]
        self.thinking_steps = []
        self.certainty_scores = []

        # æ­¥éª¤1: æ·±åº¦æ€è€ƒè¿‡ç¨‹
        initial_symptoms = [chief_complaint]
        final_certainty, final_symptoms = await self.deep_think(
            initial_symptoms,
            max_depth=self.config.thinking_depth
        )

        self.thought_process.append(f"æ·±åº¦æ€è€ƒå®Œæˆ | Deep thinking completed: {len(self.thinking_steps)} steps")
        self.thought_process.append(f"æœ€ç»ˆç¡®å®šæ€§ | Final certainty: {final_certainty:.2f}")

        # æ­¥éª¤2: ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œè¯Šæ–­
        diagnosis = await self.model_based_diagnosis(final_symptoms)

        # ç¿»è¯‘ç–¾ç—…åç§°ç”¨äºæ€è€ƒè¿‡ç¨‹
        disease_en = await self.config.translate_to_english(diagnosis['disease'])
        self.thought_process.append(
            f"æ¨¡å‹è¯Šæ–­ | Model diagnosis: {diagnosis['disease']}/{disease_en} (ç½®ä¿¡åº¦ | Confidence: {diagnosis['confidence'] * 100:.1f}%)")

        # æ­¥éª¤3: æ£€æŸ¥ç½®ä¿¡åº¦æ˜¯å¦è¾¾åˆ°é˜ˆå€¼
        if diagnosis['confidence'] < self.config.diagnosis_threshold:
            self.thought_process.append(
                f"ç½®ä¿¡åº¦ä½äºé˜ˆå€¼ {self.config.diagnosis_threshold * 100}%ï¼Œè¯·æ±‚æ›´å¤šä¿¡æ¯ | Confidence below threshold, requesting more information")
            return await self.request_more_info(chief_complaint, diagnosis['confidence'])

        # æ­¥éª¤4: çŸ¥è¯†åº“éªŒè¯
        kb_match = await self.check_knowledge_base_match(diagnosis['disease'])
        self.thought_process.append(
            f"çŸ¥è¯†åº“åŒ¹é… | Knowledge base match: {kb_match['match']} (ç›¸ä¼¼åº¦ | Similarity: {kb_match['similarity'] * 100:.1f}%)")

        # æ­¥éª¤5: ç”¨è¯æ¨è
        medication_response = await self.recommend_medication(diagnosis['disease'])

        # æ­¥éª¤6: æ£€æŸ¥å»ºè®®
        test_recommendation = await self.recommend_tests(diagnosis['disease'])

        # æ­¥éª¤7: å¯è§†åŒ–æ€è€ƒè¿‡ç¨‹
        thinking_viz = self.training_monitor.plot_thinking_process(
            [f"Step {i + 1}" for i in range(len(self.thinking_steps))],
            self.certainty_scores
        )
        self.thought_process.append(f"æ€è€ƒè¿‡ç¨‹å¯è§†åŒ–å·²ä¿å­˜ | Thinking process visualization saved: {thinking_viz}")

        # æ­¥éª¤8: ç”Ÿæˆæœ€ç»ˆå“åº”
        return await self.generate_final_response(
            diagnosis,
            kb_match,
            medication_response,
            test_recommendation
        )

    async def model_based_diagnosis(self, symptoms):
        """ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œè¯Šæ–­ (Diagnosis using trained model)"""
        # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œä¼šè°ƒç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œè¯Šæ–­
        # ç®€åŒ–ç‰ˆï¼šåŸºäºçŸ¥è¯†åº“çš„è§„åˆ™åŒ¹é…

        # é¦–å…ˆå°†ç—‡çŠ¶åˆ—è¡¨è½¬æ¢ä¸ºæ–‡æœ¬
        symptoms_text = ", ".join(symptoms)
        symptoms_en = await self.config.translate_to_english(symptoms_text)

        best_match = None
        best_score = 0

        # åœ¨çŸ¥è¯†åº“ä¸­å¯»æ‰¾æœ€åŒ¹é…çš„ç–¾ç—…
        for disease, info in self.knowledge_base.disease_info.items():
            # è·å–ç–¾ç—…çš„ç—‡çŠ¶
            disease_symptoms = info.get("symptoms", [])

            # è®¡ç®—åŒ¹é…åˆ†æ•°
            score = await self.calculate_symptom_match(symptoms_text, disease_symptoms)

            if score > best_score:
                best_score = score
                best_match = disease

        # å¦‚æœæ‰¾åˆ°åŒ¹é…çš„ç–¾ç—…ä¸”ç½®ä¿¡åº¦è¶³å¤Ÿé«˜
        if best_match and best_score > 0.6:
            return {
                "disease": best_match,
                "confidence": min(best_score, 0.95)  # æœ€é«˜95%ï¼Œä¿ç•™æ”¹è¿›ç©ºé—´
            }
        else:
            # é»˜è®¤è¿”å›é«˜è¡€å‹ï¼Œç½®ä¿¡åº¦è¾ƒä½
            return {
                "disease": "é«˜è¡€å‹",
                "confidence": 0.75  # é»˜è®¤ç½®ä¿¡åº¦
            }

    async def calculate_symptom_match(self, complaint, symptoms):
        """è®¡ç®—ç—‡çŠ¶åŒ¹é…åº¦ (Calculate symptom match score)"""
        if not symptoms:
            return 0.0

        # å°†ä¸»è¯‰å’Œç—‡çŠ¶éƒ½ç¿»è¯‘ä¸ºè‹±æ–‡è¿›è¡Œæ¯”è¾ƒ
        complaint_en = await self.config.translate_to_english(complaint) or complaint.lower()

        total_score = 0
        count = 0

        for symptom in symptoms:
            symptom_en = await self.config.translate_to_english(symptom) or symptom.lower()

            # ä½¿ç”¨ç¼–è¾‘è·ç¦»è®¡ç®—ç›¸ä¼¼åº¦
            similarity = 1 - (Levenshtein.distance(complaint_en, symptom_en) /
                              max(len(complaint_en), len(symptom_en)))

            # å¦‚æœç›¸ä¼¼åº¦è¶…è¿‡é˜ˆå€¼ï¼Œè®¡å…¥æ€»åˆ†
            if similarity > 0.5:
                total_score += similarity
                count += 1

        # è¿”å›å¹³å‡ç›¸ä¼¼åº¦
        return total_score / count if count > 0 else 0.0

    async def check_knowledge_base_match(self, disease):
        """æ£€æŸ¥çŸ¥è¯†åº“åŒ¹é…åº¦ (Check knowledge base match)"""
        # è·å–çŸ¥è¯†åº“ä¸­æ‰€æœ‰ç–¾ç—…
        kb_diseases = list(self.knowledge_base.disease_info.keys())

        if not kb_diseases:
            return {"match": "çŸ¥è¯†åº“ä¸­æ— ç›¸å…³ç–¾ç—…ä¿¡æ¯ | No relevant disease information in knowledge base",
                    "similarity": 0.0}

        # è®¡ç®—ç›¸ä¼¼åº¦
        best_match = ""
        best_similarity = 0.0

        for kb_disease in kb_diseases:
            # ä½¿ç”¨å¤šè¯­è¨€ç›¸ä¼¼åº¦è®¡ç®—
            similarity = await self.calculate_multilingual_similarity(disease, kb_disease)
            if similarity > best_similarity:
                best_similarity = similarity
                best_match = kb_disease

        return {
            "match": best_match if best_similarity > 0.7 else "æ— åŒ¹é…ç–¾ç—… | No matching disease",
            "similarity": best_similarity
        }

    async def calculate_multilingual_similarity(self, text1, text2):
        """å¤šè¯­è¨€ç›¸ä¼¼åº¦è®¡ç®— (Multilingual similarity calculation)"""
        if not text1 or not text2:
            return 0.0

        text1_en = await self.config.translate_to_english(text1) or ""
        text2_en = await self.config.translate_to_english(text2) or ""

        if not text1_en or not text2_en:
            return 0.0

        return 1 - (Levenshtein.distance(text1_en, text2_en) / max(len(text1_en), len(text2_en)))

    async def request_more_info(self, chief_complaint, confidence):
        """è¯·æ±‚æ›´å¤šç—‡çŠ¶ä¿¡æ¯ (Request more symptom information)"""
        # è‹±æ–‡å“åº”
        en_response = f"Preliminary analysis based on: '{chief_complaint}'\n"
        en_response += f"Current confidence: {confidence * 100:.1f}% (below {self.config.diagnosis_threshold * 100}% threshold)\n"
        en_response += "Please provide more detailed symptoms for accurate diagnosis."

        # ä¸­æ–‡å“åº”
        cn_response = f"åŸºäºåˆæ­¥åˆ†æ: '{chief_complaint}'\n"
        cn_response += f"å½“å‰ç½®ä¿¡åº¦: {confidence * 100:.1f}% (ä½äº {self.config.diagnosis_threshold * 100}% é˜ˆå€¼)\n"
        cn_response += "è¯·æä¾›æ›´è¯¦ç»†çš„ç—‡çŠ¶ä»¥è¿›è¡Œå‡†ç¡®è¯Šæ–­ã€‚"

        # æ·»åŠ æ€è€ƒè¿‡ç¨‹
        thought_header = "\n\n=== æ€è€ƒè¿‡ç¨‹ | THINKING PROCESS ===\n" + "\n".join(self.thought_process)

        return await self.config.translate_bilingual(en_response, cn_response) + thought_header

    async def recommend_medication(self, disease):
        """æ¨èè¯ç‰© (ç±»äººè„‘æ€è€ƒè¿‡ç¨‹) (Recommend medication - deep thinking)"""
        disease_en = await self.config.translate_to_english(disease)
        self.thought_process.append(
            f"ä¸º {disease}/{disease_en} æ¨èè¯ç‰© | Recommending medication for {disease}/{disease_en}...")

        # è·å–çŸ¥è¯†åº“ä¸­çš„è¯ç‰©
        medications = self.knowledge_base.disease_info.get(disease, {}).get("medications", [])

        if not medications:
            return {"status": "no_medication",
                    "message": "çŸ¥è¯†åº“ä¸­æ— ç›¸å…³è¯ç‰©ä¿¡æ¯ | No medication information in knowledge base"}

        # è®¡ç®—DDDå€¼
        results = []
        total_ddd = 0.0

        for med in medications:
            ddd_value, alternatives = await self.knowledge_base.calculate_ddd(
                med["name"], med["specification"]
            )

            if ddd_value is None:  # éœ€è¦æ¢è¯æˆ–æ— æ³•è®¡ç®—DDD
                if alternatives and isinstance(alternatives, list) and len(alternatives) > 0:
                    # å¦‚æœæ˜¯æ›¿ä»£è¯ç‰©åˆ—è¡¨
                    alt_text = ", ".join([f"{alt['name']} ({alt['specification']})" for alt in alternatives[:3]])
                    results.append({
                        "medication": med["name"],
                        "specification": med["specification"],
                        "status": "need_alternative",
                        "message": f"æ— æ³•è®¡ç®—DDDï¼Œå»ºè®®æ¢è¯ | Cannot calculate DDD, suggested alternatives: {alt_text}"
                    })
                elif alternatives and isinstance(alternatives, str):
                    # å¦‚æœæ˜¯é”™è¯¯æ¶ˆæ¯
                    results.append({
                        "medication": med["name"],
                        "specification": med["specification"],
                        "status": "no_ddd",
                        "message": alternatives  # ä½¿ç”¨è¿”å›çš„é”™è¯¯æ¶ˆæ¯
                    })
                else:
                    results.append({
                        "medication": med["name"],
                        "specification": med["specification"],
                        "status": "no_ddd",
                        "message": "æ— æ³•è®¡ç®—DDDä¸”æ— æ›¿ä»£è¯ç‰© | Cannot calculate DDD and no alternatives found"
                    })
            else:
                results.append({
                    "medication": med["name"],
                    "specification": med["specification"],
                    "ddd": ddd_value,
                    "status": "success"
                })
                total_ddd += ddd_value

        return {
            "status": "success" if any(r["status"] == "success" for r in results) else "partial",
            "medications": results,
            "total_ddd": total_ddd
        }

    async def recommend_tests(self, disease):
        """æ¨èæ£€æŸ¥ (åŸºäºçŸ¥è¯†åº“çš„æ·±åº¦æ€è€ƒ) (Recommend tests - deep thinking)"""
        disease_en = await self.config.translate_to_english(disease)
        self.thought_process.append(
            f"ä¸º {disease}/{disease_en} åˆ†ææ£€æŸ¥éœ€æ±‚ | Analyzing test requirements for {disease}/{disease_en}...")

        # é¦–å…ˆæ£€æŸ¥çŸ¥è¯†åº“ä¸­æ˜¯å¦æœ‰è¯¥ç–¾ç—…çš„ç›¸å…³ä¿¡æ¯
        disease_info = self.knowledge_base.disease_info.get(disease, {})

        if not disease_info:
            # çŸ¥è¯†åº“ä¸­æ²¡æœ‰è¯¥ç–¾ç—…ä¿¡æ¯
            self.thought_process.append(
                f"çŸ¥è¯†åº“ä¸­æ²¡æœ‰å…³äº {disease}/{disease_en} çš„ä¿¡æ¯ | No information about {disease}/{disease_en} in knowledge base")
            return None

        # æ£€æŸ¥çŸ¥è¯†åº“ä¸­æ˜¯å¦æœ‰æ˜ç¡®çš„æ£€æŸ¥å»ºè®®
        if "tests" in disease_info and disease_info["tests"]:
            tests = disease_info["tests"]
            self.thought_process.append(
                f"ä»çŸ¥è¯†åº“ä¸­æ‰¾åˆ° {len(tests)} é¡¹æ£€æŸ¥å»ºè®® | Found {len(tests)} test recommendations in knowledge base")
            return tests

        # çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ˜ç¡®çš„æ£€æŸ¥å»ºè®®ï¼Œå°è¯•ä»ç—‡çŠ¶ä¸­æ¨æ–­
        symptoms = disease_info.get("symptoms", [])
        inferred_tests = await self.infer_tests_from_symptoms(symptoms)

        if inferred_tests:
            self.thought_process.append(
                f"ä» {len(symptoms)} ä¸ªç—‡çŠ¶æ¨æ–­å‡º {len(inferred_tests)} é¡¹æ£€æŸ¥ | Inferred {len(inferred_tests)} tests from {len(symptoms)} symptoms")
            return inferred_tests

        # æ²¡æœ‰ä»»ä½•å¯ç”¨çš„æ£€æŸ¥å»ºè®®
        self.thought_process.append(
            f"æ— æ³•ä¸º {disease}/{disease_en} æ¨èä»»ä½•æ£€æŸ¥ | Unable to recommend any tests for {disease}/{disease_en}")
        return None

    async def infer_tests_from_symptoms(self, symptoms):
        """ä»ç—‡çŠ¶æ¨æ–­æ£€æŸ¥é¡¹ç›® (åŸºäºçŸ¥è¯†åº“çš„æ·±åº¦æ€è€ƒ) (Infer tests from symptoms - deep thinking)"""
        if not symptoms:
            return []

        # ä»çŸ¥è¯†åº“ä¸­æ”¶é›†æ‰€æœ‰ç—‡çŠ¶ç›¸å…³çš„æ£€æŸ¥
        symptom_test_mapping = {}
        for symptom, info in self.knowledge_base.symptom_info.items():
            if "related_tests" in info:
                symptom_test_mapping[symptom] = info["related_tests"]

        # æ‰¾å‡ºå½“å‰ç—‡çŠ¶ç›¸å…³çš„æ£€æŸ¥
        recommended_tests = []
        for symptom in symptoms:
            # åœ¨çŸ¥è¯†åº“ä¸­æŸ¥æ‰¾æœ€åŒ¹é…çš„ç—‡çŠ¶
            best_match = symptom
            max_similarity = 0
            for kb_symptom in symptom_test_mapping.keys():
                similarity = await self.calculate_symptom_similarity(symptom, kb_symptom)
                if similarity > max_similarity:
                    max_similarity = similarity
                    best_match = kb_symptom

            # å¦‚æœæ‰¾åˆ°åŒ¹é…çš„ç—‡çŠ¶ä¸”æœ‰ç›¸å…³æ£€æŸ¥ï¼Œåˆ™æ·»åŠ 
            if max_similarity > 0.7 and best_match in symptom_test_mapping:
                recommended_tests.extend(symptom_test_mapping[best_match])

        # å»é‡å¹¶é™åˆ¶æ•°é‡
        return list(set(recommended_tests))[:5]  # æœ€å¤šè¿”å›5é¡¹

    async def calculate_symptom_similarity(self, symptom1, symptom2):
        """è®¡ç®—ç—‡çŠ¶ç›¸ä¼¼åº¦ (æ”¯æŒå¤šè¯­è¨€) (Calculate symptom similarity)"""
        # ç¿»è¯‘ä¸ºè‹±æ–‡åæ¯”è¾ƒ
        symptom1_en = await self.config.translate_to_english(symptom1)
        symptom2_en = await self.config.translate_to_english(symptom2)

        # ä½¿ç”¨ç¼–è¾‘è·ç¦»è®¡ç®—ç›¸ä¼¼åº¦
        if symptom1_en and symptom2_en:
            return 1 - (Levenshtein.distance(symptom1_en, symptom2_en) / max(len(symptom1_en), len(symptom2_en)))
        return 0.0

    async def generate_final_response(self, diagnosis, kb_match, medication, tests):
        """ç”Ÿæˆæœ€ç»ˆå“åº” (æ ¼å¼ä¸å›¾ç‰‡ä¸€è‡´ï¼Œä½†åŒ…å«ä¸­è‹±æ–‡) (Generate final response matching the image format with bilingual content)"""
        # ä¸­æ–‡éƒ¨åˆ† (æŒ‰ç…§å›¾ç‰‡æ ¼å¼)
        cn_response = f"è¯Šæ–­ ===\n"
        cn_response += f"æ¨¡å‹:1Lama2\n"
        cn_response += f"ç–¾ç—…: {diagnosis['disease']}\n"
        cn_response += f"ç½®ä¿¡åº¦: {diagnosis['confidence'] * 100:.1f}%\n"
        cn_response += f"çŸ¥è¯†åº“åŒ¹é…: {kb_match['match']} (ç›¸ä¼¼åº¦: {kb_match['similarity'] * 100:.1f}%)\n\n"

        # è‹±æ–‡éƒ¨åˆ†
        en_disease = await self.config.translate_to_english(diagnosis['disease'])
        en_match = await self.config.translate_to_english(kb_match['match'])

        en_response = f"Diagnosis ===\n"
        en_response += f"Model:1Lama2\n"
        en_response += f"Disease: {en_disease}\n"
        en_response += f"Confidence: {diagnosis['confidence'] * 100:.1f}%\n"
        en_response += f"Knowledge Base Match: {en_match} (Similarity: {kb_match['similarity'] * 100:.1f}%)\n\n"

        # æ·»åŠ çŸ¥è¯†åº“æ‘˜è¦ (ä¸­æ–‡)
        cn_response += "=== çŸ¥è¯†åº“æ‘˜è¦ ===\n"
        cn_response += f"æ€»æ–‡æ¡£æ•°: {len(self.knowledge_base.full_knowledge)}\n"
        cn_response += f"æ€»å¤§å°: {self.knowledge_base.learning_stats['total_size_kb']:.2f} KB\n"
        cn_response += f"æå–ç–¾ç—…æ•°: {self.knowledge_base.learning_stats['diseases_extracted']}\n\n"

        # æ·»åŠ çŸ¥è¯†åº“æ‘˜è¦ (è‹±æ–‡)
        en_response += "=== Knowledge Base Summary ===\n"
        en_response += f"Total documents: {len(self.knowledge_base.full_knowledge)}\n"
        en_response += f"Total size: {self.knowledge_base.learning_stats['total_size_kb']:.2f} KB\n"
        en_response += f"Diseases extracted: {self.knowledge_base.learning_stats['diseases_extracted']}\n\n"

        # è¯ç‰©æ¨è (ä¸­æ–‡)
        cn_response += "è¯ç‰©æ¨è:\n"
        if medication["status"] == "no_medication":
            cn_response += "çŸ¥è¯†åº“ä¸­æœªæ‰¾åˆ°è¯ç‰©ä¿¡æ¯\n"
        else:
            for med in medication["medications"]:
                if med["status"] == "success":
                    cn_response += f"- {med['medication']}: {med['specification']} (DDDå€¼: {med['ddd']:.2f})\n"
                elif med["status"] == "need_alternative":
                    cn_response += f"- {med['medication']}: {med['message']}\n"

            if medication["total_ddd"] > 0:
                cn_response += f"æ€»DDDå€¼: {medication['total_ddd']:.2f}\n"

        # è¯ç‰©æ¨è (è‹±æ–‡)
        en_response += "Medication Recommendations:\n"
        if medication["status"] == "no_medication":
            en_response += "No medication information found in knowledge base\n"
        else:
            for med in medication["medications"]:
                if med["status"] == "success":
                    en_med = await self.config.translate_to_english(med['medication'])
                    en_spec = await self.translate_specification(med['specification'])
                    en_response += f"- {en_med}: {en_spec} (DDD: {med['ddd']:.2f})\n"
                elif med["status"] == "need_alternative":
                    en_med = await self.config.translate_to_english(med['medication'])
                    en_msg = await self.config.translate_to_english(med['message'])
                    en_response += f"- {en_med}: {en_msg}\n"

            if medication["total_ddd"] > 0:
                en_response += f"Total DDD: {medication['total_ddd']:.2f}\n"

        # æ¨èæ£€æŸ¥ (ä¸­æ–‡)
        cn_response += "\næ¨èæ£€æŸ¥:\n"
        if tests:
            for test in tests:
                cn_response += f"- {test}\n"
        else:
            cn_response += "åŸºäºå½“å‰çŸ¥è¯†åº“ï¼Œæš‚æ— ç‰¹å®šæ£€æŸ¥å»ºè®®"

        # æ¨èæ£€æŸ¥ (è‹±æ–‡)
        en_response += "\nRecommended Tests:\n"
        if tests:
            for test in tests:
                en_test = await self.config.translate_to_english(test)
                en_response += f"- {en_test}\n"
        else:
            en_response += "No specific tests recommended based on current knowledge"

        # æ·»åŠ æ€è€ƒè¿‡ç¨‹
        thought_header = "\n\n=== æ€è€ƒè¿‡ç¨‹ | THINKING PROCESS ===\n" + "\n".join(self.thought_process)

        # ç»„åˆä¸­è‹±æ–‡å“åº”
        final_response = f"ğŸŒ ä¸­æ–‡ | CHINESE:\n{cn_response}\n\n"
        final_response += f"ğŸŒ ENGLISH:\n{en_response}\n\n"
        final_response += thought_header

        return final_response

    async def translate_specification(self, specification):
        """ç¿»è¯‘è¯å“è§„æ ¼ (Translate medication specification)"""
        # ç®€å•çš„å•ä½ç¿»è¯‘æ˜ å°„
        unit_mapping = {
            "ç‰‡": "tablet",
            "ç²’": "capsule",
            "æ¯«å…‹": "mg",
            "æ¯«å‡": "ml",
            "/": "/"
        }

        # ä¿ç•™æ•°å­—å’Œç‰¹æ®Šå­—ç¬¦ï¼Œç¿»è¯‘ä¸­æ–‡å•ä½
        translated = specification
        for cn, en in unit_mapping.items():
            translated = translated.replace(cn, en)

        return translated


# ========== è®­ç»ƒå‡½æ•° ==========
async def train_model(config, knowledge_base):
    """è®­ç»ƒåŒ»ç–—è¯Šæ–­æ¨¡å‹"""
    logger.info("å¼€å§‹è®­ç»ƒåŒ»ç–—è¯Šæ–­æ¨¡å‹ | Starting medical diagnosis model training...")

    # è¿™é‡Œåº”è¯¥æ˜¯å®é™…çš„æ•°æ®å‡†å¤‡å’Œè®­ç»ƒè¿‡ç¨‹
    # ç®€åŒ–ç‰ˆï¼šæ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹

    monitor = TrainingMonitor(config)

    # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹
    for epoch in range(config.epochs):
        # æ¨¡æ‹Ÿè®­ç»ƒæŒ‡æ ‡
        train_loss = 0.5 * (0.9 ** epoch)
        val_loss = 0.6 * (0.9 ** epoch)
        train_acc = 0.7 + 0.2 * (1 - 0.9 ** epoch)
        val_acc = 0.65 + 0.2 * (1 - 0.9 ** epoch)
        lr = config.learning_rate * (0.95 ** epoch)

        # æ›´æ–°ç›‘æ§å™¨
        monitor.update(train_loss, val_loss, train_acc, val_acc, lr)

        logger.info(f"Epoch {epoch + 1}/{config.epochs} - "
                    f"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, "
                    f"Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}")

        # æ¨¡æ‹ŸGPUè®­ç»ƒ
        await asyncio.sleep(0.5)  # æ¨¡æ‹Ÿè®­ç»ƒæ—¶é—´

    # ä¿å­˜å­¦ä¹ æ›²çº¿
    learning_curve_path = monitor.plot_learning_curves()
    logger.info(f"å­¦ä¹ æ›²çº¿å·²ä¿å­˜ | Learning curves saved: {learning_curve_path}")

    # ä¿å­˜æ¨¡å‹
    model_path = os.path.join(config.model_dir, "diagnosis_model.pth")
    # è¿™é‡Œåº”è¯¥æ˜¯å®é™…çš„æ¨¡å‹ä¿å­˜ä»£ç 
    # torch.save(model.state_dict(), model_path)
    logger.info(f"æ¨¡å‹å·²ä¿å­˜ | Model saved: {model_path}")

    return monitor


# ========== ä¸»å‡½æ•° ==========
async def main():
    try:
        # åˆå§‹åŒ–é…ç½®
        config = MedicalConfig()

        # åŠ è½½çŸ¥è¯†åº“
        logger.info("\n[1/3] åŠ è½½åŒ»ç–—çŸ¥è¯† | Loading medical knowledge...")
        knowledge_base = MedicalKnowledgeBase(config)

        # è®­ç»ƒæ¨¡å‹
        logger.info("\n[2/3] è®­ç»ƒåŒ»ç–—è¯Šæ–­æ¨¡å‹ | Training medical diagnosis model...")
        monitor = await train_model(config, knowledge_base)

        # åˆå§‹åŒ–åŒ»ç–—åŠ©æ‰‹
        logger.info("\n[3/3] å¯åŠ¨åŒ»ç–—åŠ©æ‰‹ | Starting Medical Assistant")
        assistant = MedicalAssistant(knowledge_base, config)

        # äº¤äº’ç•Œé¢
        logger.info("\n=== åŒ»ç–—è¯Šæ–­åŠ©æ‰‹ (åŒ»ç”Ÿç‰ˆ) | MEDICAL DIAGNOSTIC ASSISTANT (FOR PHYSICIANS) ===")
        logger.info("è¾“å…¥æ‚£è€…ç—‡çŠ¶è¿›è¡Œè¯Šæ–­æˆ–è¾“å…¥'exit'é€€å‡º | Enter patient symptoms for diagnosis or 'exit' to quit")
        logger.info(f"è¯Šæ–­é˜ˆå€¼ | Diagnosis threshold: {config.diagnosis_threshold * 100}%")
        logger.info("æ”¯æŒä¸­è‹±æ–‡è¾“å…¥ | Supports Chinese and English input")

        while True:
            user_input = input("\nè¾“å…¥ç—‡çŠ¶ | Enter symptoms: ").strip()

            if user_input.lower() == "exit":
                break

            response = await assistant.diagnose(user_input)
            print(f"\n{response}")

    except Exception as e:
        error_msg = f"ç³»ç»Ÿé”™è¯¯ | System error: {str(e)}"
        logger.error(error_msg)
        print(error_msg)


if __name__ == "__main__":
    asyncio.run(main())